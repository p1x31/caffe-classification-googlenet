{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.97       300\n",
      "           1       0.87      1.00      0.93       300\n",
      "           2       1.00      0.92      0.96       300\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       900\n",
      "   macro avg       0.96      0.95      0.95       900\n",
      "weighted avg       0.96      0.95      0.95       900\n",
      " samples avg       0.95      0.95      0.95       900\n",
      "\n",
      "mAP: 0.9674074074074074\n",
      "Total number of parameters: 50147\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))\n",
    "#print(fps_adam)\n",
    "print('mAP: ' + mAP_adam)\n",
    "print (\"Total number of parameters: \" + str(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from numpy import prod\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "\n",
    "from itertools import product\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "import warnings\n",
    "\"\"\"Optional[...] is a shorthand notation for Union[..., None], \n",
    "telling the type checker that either an object of the specific type is required, \n",
    "or None is required. ... stands for any valid type hint, \n",
    "including complex compound types or a Union[] of more types. \n",
    "Whenever you have a keyword argument with default value None, \n",
    "you should use Optional.\"\"\"\n",
    "from torch.jit.annotations import Optional, Tuple\n",
    "\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "model = models.googlenet(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): BasicConv2d(\n",
      "    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  (2): BasicConv2d(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (lrn): LocalResponseNorm(5, alpha=9.9999997e-05, beta=0.75, k=1.0)\n",
      "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(*list(model.children())[0:3])\n",
    "model.lrn = nn.LocalResponseNorm(5, alpha = 9.9999997e-05)\n",
    "model.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True, padding=0, dilation=1)\n",
    "#[(W−K+2P)/S]+1\n",
    "#model = nn.Sequential(*[model[i] for i in range(4)])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, pool_proj,\n",
    "                 conv_block=None):\n",
    "        super(Inception, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "            \n",
    "        self.branch1 = conv_block(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = conv_block(in_channels, ch3x3red, kernel_size=1)\n",
    "\n",
    "\n",
    "    def _forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "\n",
    "\n",
    "        outputs = [branch1, branch2]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x, inplace=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [BasicConv2d, Inception]\n",
    "conv_block = blocks[0]\n",
    "inception_block = blocks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inception3a = inception_block(64, 8, 16, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [128, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2        [128, 64, 112, 112]             128\n",
      "       BasicConv2d-3        [128, 64, 112, 112]               0\n",
      "         MaxPool2d-4          [128, 64, 56, 56]               0\n",
      "            Conv2d-5          [128, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6          [128, 64, 56, 56]             128\n",
      "       BasicConv2d-7          [128, 64, 56, 56]               0\n",
      " LocalResponseNorm-8          [128, 64, 56, 56]               0\n",
      "         MaxPool2d-9          [128, 64, 28, 28]               0\n",
      "           Conv2d-10           [128, 8, 28, 28]             512\n",
      "      BasicConv2d-11           [128, 8, 28, 28]               0\n",
      "           Conv2d-12          [128, 16, 28, 28]           1,024\n",
      "      BasicConv2d-13          [128, 16, 28, 28]               0\n",
      "        Inception-14          [128, 24, 28, 28]               0\n",
      "AdaptiveAvgPool2d-15          [128, 24, 22, 22]               0\n",
      "          Dropout-16          [128, 24, 22, 22]               0\n",
      "          Flatten-17               [128, 11616]               0\n",
      "           Linear-18                   [128, 3]          34,851\n",
      "       LogSoftmax-19                   [128, 3]               0\n",
      "================================================================\n",
      "Total params: 50,147\n",
      "Trainable params: 36,387\n",
      "Non-trainable params: 13,760\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 73.50\n",
      "Forward/backward pass size (MB): 3470.16\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 3543.85\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.avgpool = nn.AdaptiveAvgPool2d(output_size=(22, 22))\n",
    "model.dropout = nn.Dropout(p = 0.20000001)\n",
    "model.fc = nn.Sequential(OrderedDict([\n",
    "    ('flatten', nn.Flatten()),\n",
    "    ('fc1', nn.Linear(11616, 3)),\n",
    "    ('output', nn.LogSoftmax(dim = 1))\n",
    "    ]))\n",
    "model = model.to(device)\n",
    "summary(model, input_size=(3, 224, 224), batch_size=128, device = str(torch.device(\"cuda\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BasicConv2d(\n",
       "    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (2): BasicConv2d(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (lrn): LocalResponseNorm(5, alpha=9.9999997e-05, beta=0.75, k=1.0)\n",
       "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (inception3a): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (branch2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(22, 22))\n",
       "  (dropout): Dropout(p=0.20000001, inplace=False)\n",
       "  (fc): Sequential(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (fc1): Linear(in_features=11616, out_features=3, bias=True)\n",
       "    (output): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(*list(model.children())[3:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): LocalResponseNorm(5, alpha=9.9999997e-05, beta=0.75, k=1.0)\n",
       "  (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (2): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (branch2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (3): AdaptiveAvgPool2d(output_size=(22, 22))\n",
       "  (4): Dropout(p=0.20000001, inplace=False)\n",
       "  (5): Sequential(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (fc1): Linear(in_features=11616, out_features=3, bias=True)\n",
       "    (output): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "      return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the hyper-parameters and return a Run namedtuple containing all the \n",
    "# combinations of hyper-parameters\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "        \n",
    "        Run = namedtuple(\"Run\", params.keys())\n",
    "        \n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "            \n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper class, help track loss, accuracy, epoch time, run time, \n",
    "# hyper-parameters etc. Also record to TensorBoard and write into csv, json!@!\n",
    "class RunManager():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.epoch_count = 0\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "        self.epoch_start_time = None\n",
    "        \n",
    "        # tracking every run count, run data, hyper-params used, time\n",
    "        self.run_params = None\n",
    "        self.run_count = 0\n",
    "        self.run_data = []\n",
    "        self.run_start_time = None\n",
    "        \n",
    "        self.network = None\n",
    "        self.loader = None\n",
    "        self.tb = None\n",
    "    \n",
    "    def begin_run(self, run, network, loader):\n",
    "        \n",
    "        self.run_start_time = time.time()\n",
    "        \n",
    "        self.run_params = run\n",
    "        self.run_count += 1\n",
    "        \n",
    "        self.network = network\n",
    "        self.loader = loader\n",
    "        self.tb = SummaryWriter(comment = f'-{run}')\n",
    "        \n",
    "        images, labels = next(iter(self.loader))\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "        \n",
    "        #self.tb.add_image('images', grid)\n",
    "        #self.tb.add_graph(self.network, images)\n",
    "        \n",
    "    def end_run(self, model):\n",
    "        \n",
    "        MODEL_PATH = \"./weights/model_{}.pt\".format(self)\n",
    "        torch.save(model, MODEL_PATH)\n",
    "        \n",
    "        self.tb.close()\n",
    "        self.epoch_count = 0\n",
    "       \n",
    "       # zero epoch count, loss, accuracy, \n",
    "    def begin_epoch(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "        self.epoch_count += 1\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "        \n",
    "    # \n",
    "    def end_epoch(self, model):\n",
    "        # calculate epoch duration and run duration(accumulate)\n",
    "        epoch_duration = time.time() - self.epoch_start_time\n",
    "        run_duration = time.time() - self.run_start_time\n",
    "\n",
    "        # record epoch loss and accuracy\n",
    "        loss = self.epoch_loss / len(self.loader.dataset)\n",
    "        accuracy = self.epoch_num_correct / len(self.loader.dataset)\n",
    "\n",
    "        # Record epoch loss and accuracy to TensorBoard \n",
    "        self.tb.add_scalar('Loss', loss, self.epoch_count)\n",
    "        self.tb.add_scalar('Accuracy', accuracy, self.epoch_count)\n",
    "\n",
    "        # Record params to TensorBoard\n",
    "       # for name, param in self.network.named_parameters():\n",
    "       #     self.tb.add_histogram(name, param, self.epoch_count)\n",
    "       #     self.tb.add_histogram(f'{name}.grad', param.grad, self.epoch_count)\n",
    "    \n",
    "\n",
    "        # Write into 'results' (OrderedDict) for all run related data\n",
    "        results = OrderedDict()\n",
    "        results[\"run\"] = self.run_count\n",
    "        results[\"epoch\"] = self.epoch_count\n",
    "        results[\"loss\"] = loss\n",
    "        results[\"accuracy\"] = accuracy\n",
    "        results[\"epoch duration\"] = epoch_duration\n",
    "        results[\"run duration\"] = run_duration\n",
    "        \n",
    "        MODEL_PATH = \"./weights/model_{}.pt\".format(self.epoch_count)\n",
    "        torch.save(model, MODEL_PATH)\n",
    "        \n",
    "        # Record hyper-params into 'results'\n",
    "        for k,v in self.run_params._asdict().items(): results[k] = v\n",
    "        self.run_data.append(results)\n",
    "        df = pd.DataFrame.from_dict(self.run_data, orient = 'columns')\n",
    "\n",
    "        # display epoch information and show progress\n",
    "        clear_output(wait=True)\n",
    "        display(df)\n",
    "\n",
    "      # accumulate loss of batch into entire epoch loss\n",
    "    def track_loss(self, loss):\n",
    "        # multiply batch size so variety of batch sizes can be compared\n",
    "        self.epoch_loss += loss.item() * self.loader.batch_size\n",
    "\n",
    "      # accumulate number of corrects of batch into entire epoch num_correct\n",
    "    def track_num_correct(self, preds, labels):\n",
    "        self.epoch_num_correct += self._get_num_correct(preds, labels)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _get_num_correct(self, preds, lables):\n",
    "        return preds.argmax(dim = 1).eq(labels).sum().item()\n",
    "    \n",
    "    def save(self, fileName):\n",
    "        \n",
    "        pd.DataFrame.from_dict(\n",
    "        self.run_data,\n",
    "        orient = \"columns\").to_csv(f'{fileName}.csv')\n",
    "        \n",
    "        \n",
    "        \n",
    "        with open(f'{fileName}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.run_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.ImageFolder(\n",
    "    root = './Data_cleaned/train'\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 246 ms, sys: 149 ms, total: 395 ms\n",
      "Wall time: 7.53 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.4738), tensor(0.2598))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size = 512, num_workers = 2, pin_memory = True\n",
    ")\n",
    "data = next(iter(loader))\n",
    "mean = data[0].mean()\n",
    "std = data[0].std()\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_normal = torchvision.datasets.ImageFolder(\n",
    "    root = './Data_cleaned/train'\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsets = {\n",
    "    'not_normal': train_set,\n",
    "    'normal' : train_set_normal\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = nn.Sequential(*list(model.children())[0:3])\n",
    "network.classifier = model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = {\n",
    "    'batch_norm': network\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIR = '/home/vadim/testovoe/dev/'\n",
    "test_df = pd.read_csv(CURRENT_DIR+\"Data/text.txt\", sep=\"\\s+\", header=None, names=[\"name\", \"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_width(im, new_shape, is_rgb=True):\n",
    "    pad_diff = new_shape - im.shape[0], new_shape - im.shape[1]\n",
    "    t, b = math.floor(pad_diff[0]/2), math.ceil(pad_diff[0]/2)\n",
    "    l, r = math.floor(pad_diff[1]/2), math.ceil(pad_diff[1]/2)\n",
    "    if is_rgb:\n",
    "        pad_width = ((t,b), (l,r), (0, 0))\n",
    "    else:\n",
    "        pad_width = ((t,b), (l,r))\n",
    "    return pad_width\n",
    "\n",
    "def preprocess_image(image_path, desired_size=224):\n",
    "    im = Image.open(image_path)\n",
    "    im = im.resize((desired_size, )*2, resample=Image.LANCZOS)\n",
    "    \n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:02<00:00, 414.07it/s]\n"
     ]
    }
   ],
   "source": [
    "N = test_df.shape[0]\n",
    "x_test = np.empty((N, 224, 224, 3), dtype=np.uint8)\n",
    "\n",
    "for i, image_id in enumerate(tqdm(test_df['name'])):\n",
    "    x_test[i, :, :, :] = preprocess_image(\n",
    "         f'/home/vadim/testovoe/dev/{image_id}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.get_dummies(test_df['category']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BasicConv2d(\n",
       "    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (2): BasicConv2d(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): LocalResponseNorm(5, alpha=9.9999997e-05, beta=0.75, k=1.0)\n",
       "    (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (2): Inception(\n",
       "      (branch1): BasicConv2d(\n",
       "        (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (branch2): BasicConv2d(\n",
       "        (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (3): AdaptiveAvgPool2d(output_size=(22, 22))\n",
       "    (4): Dropout(p=0.20000001, inplace=False)\n",
       "    (5): Sequential(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (fc1): Linear(in_features=11616, out_features=3, bias=True)\n",
       "      (output): LogSoftmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>num_workers</th>\n",
       "      <th>device</th>\n",
       "      <th>trainset</th>\n",
       "      <th>network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.950129</td>\n",
       "      <td>0.935096</td>\n",
       "      <td>55.798757</td>\n",
       "      <td>56.386567</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.080913</td>\n",
       "      <td>0.272820</td>\n",
       "      <td>56.180495</td>\n",
       "      <td>112.584605</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.522101</td>\n",
       "      <td>0.332233</td>\n",
       "      <td>54.493393</td>\n",
       "      <td>167.098216</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.036301</td>\n",
       "      <td>0.332572</td>\n",
       "      <td>55.348439</td>\n",
       "      <td>222.463122</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.763039</td>\n",
       "      <td>0.333066</td>\n",
       "      <td>55.392844</td>\n",
       "      <td>277.873277</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>1</td>\n",
       "      <td>295</td>\n",
       "      <td>0.048324</td>\n",
       "      <td>0.983712</td>\n",
       "      <td>53.825144</td>\n",
       "      <td>15974.429213</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>0.048999</td>\n",
       "      <td>0.983157</td>\n",
       "      <td>53.989328</td>\n",
       "      <td>16028.443078</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>1</td>\n",
       "      <td>297</td>\n",
       "      <td>0.048734</td>\n",
       "      <td>0.983311</td>\n",
       "      <td>54.265919</td>\n",
       "      <td>16082.733626</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1</td>\n",
       "      <td>298</td>\n",
       "      <td>0.047343</td>\n",
       "      <td>0.983712</td>\n",
       "      <td>53.828166</td>\n",
       "      <td>16136.586855</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>1</td>\n",
       "      <td>299</td>\n",
       "      <td>0.046136</td>\n",
       "      <td>0.984113</td>\n",
       "      <td>53.844035</td>\n",
       "      <td>16190.457201</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     run  epoch      loss  accuracy  epoch duration  run duration     lr  \\\n",
       "0      1      1  0.950129  0.935096       55.798757     56.386567  0.001   \n",
       "1      1      2  3.080913  0.272820       56.180495    112.584605  0.001   \n",
       "2      1      3  2.522101  0.332233       54.493393    167.098216  0.001   \n",
       "3      1      4  1.036301  0.332572       55.348439    222.463122  0.001   \n",
       "4      1      5  1.763039  0.333066       55.392844    277.873277  0.001   \n",
       "..   ...    ...       ...       ...             ...           ...    ...   \n",
       "294    1    295  0.048324  0.983712       53.825144  15974.429213  0.001   \n",
       "295    1    296  0.048999  0.983157       53.989328  16028.443078  0.001   \n",
       "296    1    297  0.048734  0.983311       54.265919  16082.733626  0.001   \n",
       "297    1    298  0.047343  0.983712       53.828166  16136.586855  0.001   \n",
       "298    1    299  0.046136  0.984113       53.844035  16190.457201  0.001   \n",
       "\n",
       "     batch_size  weight_decay  num_workers device trainset     network  \n",
       "0           128      0.000006            1   cuda   normal  batch_norm  \n",
       "1           128      0.000006            1   cuda   normal  batch_norm  \n",
       "2           128      0.000006            1   cuda   normal  batch_norm  \n",
       "3           128      0.000006            1   cuda   normal  batch_norm  \n",
       "4           128      0.000006            1   cuda   normal  batch_norm  \n",
       "..          ...           ...          ...    ...      ...         ...  \n",
       "294         128      0.000006            1   cuda   normal  batch_norm  \n",
       "295         128      0.000006            1   cuda   normal  batch_norm  \n",
       "296         128      0.000006            1   cuda   normal  batch_norm  \n",
       "297         128      0.000006            1   cuda   normal  batch_norm  \n",
       "298         128      0.000006            1   cuda   normal  batch_norm  \n",
       "\n",
       "[299 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# Test different configurations\n",
    "# for every run [value] that is going to be used e.g [.001, .01] = two runs\n",
    "# the bigger the batch the less gradient updates steps made\n",
    "# implement callback\n",
    "params = OrderedDict(\n",
    "    lr = [.001],\n",
    "    # max batch size 352\n",
    "    batch_size = [128],\n",
    "    weight_decay = [6.0000001e-06],\n",
    "    num_workers = [1],\n",
    "    device = [\"cuda\"],\n",
    "    trainset = [\"normal\"],\n",
    "    # try all the values in the dict network1, network2\n",
    "    network = list(networks.keys())\n",
    ")\n",
    "m = RunManager()\n",
    "# active run or current run\n",
    "# this thing detecting RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output. \n",
    "# if k = 0 in LRN layers\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for run in RunBuilder.get_runs(params):\n",
    "    \n",
    "    device = torch.device(run.device)\n",
    "    # redefine the network\n",
    "    network = networks[run.network].to(device)\n",
    "    network.train()\n",
    "    loader = DataLoader(trainsets[run.trainset], batch_size = run.batch_size, num_workers = run.num_workers, \n",
    "                       pin_memory = True) \n",
    "    optimizer = optim.Adam(network.classifier.parameters(), lr = run.lr, weight_decay = run.weight_decay, amsgrad=True) \n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[500,1000,1500], gamma=0.5)\n",
    "    \n",
    "    m.begin_run(run, network, loader)\n",
    "    for epoch in range(300):\n",
    "        m.begin_epoch()\n",
    "        for batch in loader:\n",
    "            #network.train()\n",
    "            images = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            for p in network.parameters(): p.grad = None\n",
    "            preds = network(images)\n",
    "            loss = F.cross_entropy(preds, labels)\n",
    "            #7.9\n",
    "            #optimizer.zero_grad()\n",
    "            #8 sec\n",
    "            \n",
    "            loss.backward() # Calculate gradients\n",
    "            optimizer.step() # Update Weights\n",
    "            m.track_loss(loss)\n",
    "            scheduler.step() \n",
    "            m.track_num_correct(preds, labels)\n",
    "        m.end_epoch(network)\n",
    "    m.end_run(network)\n",
    "m.save(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-78f72506224e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(m.run_data).sort_values(\"accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./weights/model_297.pt\"\n",
    "model = torch.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = {\n",
    "    'batch_norm': network,\n",
    "    'trained_net': model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>num_workers</th>\n",
       "      <th>device</th>\n",
       "      <th>trainset</th>\n",
       "      <th>network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.077972</td>\n",
       "      <td>0.988679</td>\n",
       "      <td>54.115081</td>\n",
       "      <td>54.727784</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.933191</td>\n",
       "      <td>0.941019</td>\n",
       "      <td>54.660468</td>\n",
       "      <td>109.404345</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.231535</td>\n",
       "      <td>0.941142</td>\n",
       "      <td>53.510615</td>\n",
       "      <td>162.931849</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.059162</td>\n",
       "      <td>0.979147</td>\n",
       "      <td>53.492336</td>\n",
       "      <td>216.445287</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.059811</td>\n",
       "      <td>0.979856</td>\n",
       "      <td>53.873811</td>\n",
       "      <td>270.333647</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>0.005956</td>\n",
       "      <td>0.998735</td>\n",
       "      <td>53.697124</td>\n",
       "      <td>15858.582165</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>1</td>\n",
       "      <td>297</td>\n",
       "      <td>0.005722</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>53.559353</td>\n",
       "      <td>15912.168745</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1</td>\n",
       "      <td>298</td>\n",
       "      <td>0.005811</td>\n",
       "      <td>0.998797</td>\n",
       "      <td>53.699200</td>\n",
       "      <td>15965.893756</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>1</td>\n",
       "      <td>299</td>\n",
       "      <td>0.005987</td>\n",
       "      <td>0.998797</td>\n",
       "      <td>53.634737</td>\n",
       "      <td>16019.551512</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>0.999167</td>\n",
       "      <td>53.566628</td>\n",
       "      <td>16073.144558</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     run  epoch      loss  accuracy  epoch duration  run duration      lr  \\\n",
       "0      1      1  0.077972  0.988679       54.115081     54.727784  0.0003   \n",
       "1      1      2  0.933191  0.941019       54.660468    109.404345  0.0003   \n",
       "2      1      3  0.231535  0.941142       53.510615    162.931849  0.0003   \n",
       "3      1      4  0.059162  0.979147       53.492336    216.445287  0.0003   \n",
       "4      1      5  0.059811  0.979856       53.873811    270.333647  0.0003   \n",
       "..   ...    ...       ...       ...             ...           ...     ...   \n",
       "295    1    296  0.005956  0.998735       53.697124  15858.582165  0.0003   \n",
       "296    1    297  0.005722  0.998859       53.559353  15912.168745  0.0003   \n",
       "297    1    298  0.005811  0.998797       53.699200  15965.893756  0.0003   \n",
       "298    1    299  0.005987  0.998797       53.634737  16019.551512  0.0003   \n",
       "299    1    300  0.005454  0.999167       53.566628  16073.144558  0.0003   \n",
       "\n",
       "     batch_size  weight_decay  num_workers device trainset      network  \n",
       "0           128      0.000003            1   cuda   normal  trained_net  \n",
       "1           128      0.000003            1   cuda   normal  trained_net  \n",
       "2           128      0.000003            1   cuda   normal  trained_net  \n",
       "3           128      0.000003            1   cuda   normal  trained_net  \n",
       "4           128      0.000003            1   cuda   normal  trained_net  \n",
       "..          ...           ...          ...    ...      ...          ...  \n",
       "295         128      0.000003            1   cuda   normal  trained_net  \n",
       "296         128      0.000003            1   cuda   normal  trained_net  \n",
       "297         128      0.000003            1   cuda   normal  trained_net  \n",
       "298         128      0.000003            1   cuda   normal  trained_net  \n",
       "299         128      0.000003            1   cuda   normal  trained_net  \n",
       "\n",
       "[300 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57min 13s, sys: 15min 31s, total: 1h 12min 45s\n",
      "Wall time: 4h 27min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test different configurations\n",
    "# for every run [value] that is going to be used e.g [.001, .01] = two runs\n",
    "# the bigger the batch the less gradient updates steps made\n",
    "# implement callback\n",
    "params = OrderedDict(\n",
    "    lr = [.0003],\n",
    "    # max batch size 352\n",
    "    batch_size = [128],\n",
    "    weight_decay = [3.0000001e-06],\n",
    "    num_workers = [1],\n",
    "    device = [\"cuda\"],\n",
    "    trainset = [\"normal\"],\n",
    "    # try all the values in the dict network1, network2\n",
    "    #network = list(networks.keys())\n",
    "    network = [\"trained_net\"]\n",
    ")\n",
    "m = RunManager()\n",
    "# active run or current run\n",
    "# this thing detecting RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output. \n",
    "# if k = 0 in LRN layers\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for run in RunBuilder.get_runs(params):\n",
    "    \n",
    "    device = torch.device(run.device)\n",
    "    # redefine the network\n",
    "    network = networks[run.network].to(device)\n",
    "    network.train()\n",
    "    loader = DataLoader(trainsets[run.trainset], batch_size = run.batch_size, num_workers = run.num_workers, \n",
    "                       pin_memory = True) \n",
    "    optimizer = optim.Adam(network.parameters(), lr = run.lr, weight_decay = run.weight_decay, amsgrad=True) \n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[500,1000,1500], gamma=0.5)\n",
    "    \n",
    "    m.begin_run(run, network, loader)\n",
    "    for epoch in range(300):\n",
    "        m.begin_epoch()\n",
    "        for batch in loader:\n",
    "            #network.train()\n",
    "            images = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            preds = network(images)\n",
    "            loss = F.cross_entropy(preds, labels)\n",
    "            #7.9\n",
    "            #optimizer.zero_grad()\n",
    "            #8 sec\n",
    "            for p in network.parameters(): p.grad = None\n",
    "            loss.backward() # Calculate gradients\n",
    "            optimizer.step() # Update Weights\n",
    "            m.track_loss(loss)\n",
    "            scheduler.step() \n",
    "            m.track_num_correct(preds, labels)\n",
    "        m.end_epoch(network)\n",
    "    m.end_run(network)\n",
    "m.save(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>num_workers</th>\n",
       "      <th>device</th>\n",
       "      <th>trainset</th>\n",
       "      <th>network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>0.999167</td>\n",
       "      <td>53.566628</td>\n",
       "      <td>16073.144558</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>1</td>\n",
       "      <td>292</td>\n",
       "      <td>0.005719</td>\n",
       "      <td>0.998982</td>\n",
       "      <td>53.586104</td>\n",
       "      <td>15643.860402</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>1</td>\n",
       "      <td>286</td>\n",
       "      <td>0.005670</td>\n",
       "      <td>0.998982</td>\n",
       "      <td>53.653476</td>\n",
       "      <td>15322.211100</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>1</td>\n",
       "      <td>276</td>\n",
       "      <td>0.005793</td>\n",
       "      <td>0.998982</td>\n",
       "      <td>53.477520</td>\n",
       "      <td>14785.783270</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>1</td>\n",
       "      <td>291</td>\n",
       "      <td>0.005906</td>\n",
       "      <td>0.998982</td>\n",
       "      <td>53.647564</td>\n",
       "      <td>15590.250579</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.046618</td>\n",
       "      <td>0.985378</td>\n",
       "      <td>53.531803</td>\n",
       "      <td>377.502581</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.059811</td>\n",
       "      <td>0.979856</td>\n",
       "      <td>53.873811</td>\n",
       "      <td>270.333647</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.059162</td>\n",
       "      <td>0.979147</td>\n",
       "      <td>53.492336</td>\n",
       "      <td>216.445287</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.231535</td>\n",
       "      <td>0.941142</td>\n",
       "      <td>53.510615</td>\n",
       "      <td>162.931849</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.933191</td>\n",
       "      <td>0.941019</td>\n",
       "      <td>54.660468</td>\n",
       "      <td>109.404345</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>trained_net</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     run  epoch      loss  accuracy  epoch duration  run duration      lr  \\\n",
       "299    1    300  0.005454  0.999167       53.566628  16073.144558  0.0003   \n",
       "291    1    292  0.005719  0.998982       53.586104  15643.860402  0.0003   \n",
       "285    1    286  0.005670  0.998982       53.653476  15322.211100  0.0003   \n",
       "275    1    276  0.005793  0.998982       53.477520  14785.783270  0.0003   \n",
       "290    1    291  0.005906  0.998982       53.647564  15590.250579  0.0003   \n",
       "..   ...    ...       ...       ...             ...           ...     ...   \n",
       "6      1      7  0.046618  0.985378       53.531803    377.502581  0.0003   \n",
       "4      1      5  0.059811  0.979856       53.873811    270.333647  0.0003   \n",
       "3      1      4  0.059162  0.979147       53.492336    216.445287  0.0003   \n",
       "2      1      3  0.231535  0.941142       53.510615    162.931849  0.0003   \n",
       "1      1      2  0.933191  0.941019       54.660468    109.404345  0.0003   \n",
       "\n",
       "     batch_size  weight_decay  num_workers device trainset      network  \n",
       "299         128      0.000003            1   cuda   normal  trained_net  \n",
       "291         128      0.000003            1   cuda   normal  trained_net  \n",
       "285         128      0.000003            1   cuda   normal  trained_net  \n",
       "275         128      0.000003            1   cuda   normal  trained_net  \n",
       "290         128      0.000003            1   cuda   normal  trained_net  \n",
       "..          ...           ...          ...    ...      ...          ...  \n",
       "6           128      0.000003            1   cuda   normal  trained_net  \n",
       "4           128      0.000003            1   cuda   normal  trained_net  \n",
       "3           128      0.000003            1   cuda   normal  trained_net  \n",
       "2           128      0.000003            1   cuda   normal  trained_net  \n",
       "1           128      0.000003            1   cuda   normal  trained_net  \n",
       "\n",
       "[300 rows x 13 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(m.run_data).sort_values(\"accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./weights/model_292.pt\"\n",
    "model = torch.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = torchvision.datasets.ImageFolder(\n",
    "    root = './Data/test'\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_set, batch_size = 128, num_workers = 1, \n",
    "                       pin_memory = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, optimizer, test_loader):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model : network\n",
    "        test_loader : pytorch test loader\n",
    "    Returns:\n",
    "        list : predicted values\n",
    "    \"\"\"\n",
    "    \n",
    "    model = torch.load(MODEL_PATH)\n",
    "    optimizer = optimizer\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            output = model(images)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(network.parameters(), lr = 0.0003, weight_decay = 0.000003)\n",
    "\n",
    "# get predictions\n",
    "predictions = test(torch.load(MODEL_PATH), optimizer, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDigits(lst): \n",
    "    return list(map(lambda el:el.tolist(), lst)) \n",
    "extractDigits(preds)\n",
    "flat_list = []\n",
    "flat_list = [item for sublist in preds for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = LabelBinarizer()\n",
    "preds = lb.fit_transform(flat_list)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.97       300\n",
      "           1       0.87      1.00      0.93       300\n",
      "           2       1.00      0.92      0.96       300\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       900\n",
      "   macro avg       0.96      0.95      0.95       900\n",
      "weighted avg       0.96      0.95      0.95       900\n",
      " samples avg       0.95      0.95      0.95       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9674074074074074\n"
     ]
    }
   ],
   "source": [
    "mAP_adam = str(average_precision_score(y_test, preds, average=\"samples\"))\n",
    "print(average_precision_score(y_test, preds, average=\"samples\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = sum([param.nelement() for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
