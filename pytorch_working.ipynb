{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       300\n",
      "           1       0.80      0.99      0.89       300\n",
      "           2       1.00      0.75      0.86       300\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       900\n",
      "   macro avg       0.93      0.92      0.91       900\n",
      "weighted avg       0.93      0.92      0.91       900\n",
      " samples avg       0.92      0.92      0.92       900\n",
      "\n",
      "FPS: 359.80 fps\n",
      "mAP: 0.9437037037037036\n",
      "Total number of parameters: 16499\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))\n",
    "print(fps_adam)\n",
    "print('mAP: ' + mAP_adam)\n",
    "print (\"Total number of parameters: \" + str(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from numpy import prod\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "\n",
    "from itertools import product\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "import warnings\n",
    "\"\"\"Optional[...] is a shorthand notation for Union[..., None], \n",
    "telling the type checker that either an object of the specific type is required, \n",
    "or None is required. ... stands for any valid type hint, \n",
    "including complex compound types or a Union[] of more types. \n",
    "Whenever you have a keyword argument with default value None, \n",
    "you should use Optional.\"\"\"\n",
    "from torch.jit.annotations import Optional, Tuple\n",
    "\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['GoogLeNet', 'googlenet', \"GoogLeNetOutputs\", \"_GoogLeNetOutputs\"]\n",
    "\n",
    "model_urls = {\n",
    "    # GoogLeNet ported from TensorFlow\n",
    "    'googlenet': 'https://download.pytorch.org/models/googlenet-1378be20.pth',\n",
    "}\n",
    "\n",
    "GoogLeNetOutputs = namedtuple('GoogLeNetOutputs', ['logits', 'aux_logits2', 'aux_logits1'])\n",
    "GoogLeNetOutputs.__annotations__ = {'logits': Tensor, 'aux_logits2': Optional[Tensor],\n",
    "                                    'aux_logits1': Optional[Tensor]}\n",
    "\n",
    "# Script annotations failed with _GoogleNetOutputs = namedtuple ...\n",
    "# _GoogLeNetOutputs set here for backwards compat\n",
    "_GoogLeNetOutputs = GoogLeNetOutputs\n",
    "\n",
    "\n",
    "def googlenet(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"GoogLeNet (Inception v1) model architecture from\n",
    "    `\"Going Deeper with Convolutions\" <http://arxiv.org/abs/1409.4842>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        aux_logits (bool): If True, adds two auxiliary branches that can improve training.\n",
    "            Default: *False* when pretrained is True otherwise *True*\n",
    "        transform_input (bool): If True, preprocesses the input according to the method with which it\n",
    "            was trained on ImageNet. Default: *False*\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        if 'transform_input' not in kwargs:\n",
    "            kwargs['transform_input'] = True\n",
    "        if 'aux_logits' not in kwargs:\n",
    "            kwargs['aux_logits'] = False\n",
    "        if kwargs['aux_logits']:\n",
    "            warnings.warn('auxiliary heads in the pretrained googlenet model are NOT pretrained, '\n",
    "                          'so make sure to train them')\n",
    "        original_aux_logits = kwargs['aux_logits']\n",
    "        kwargs['aux_logits'] = False\n",
    "        kwargs['init_weights'] = True\n",
    "        model = GoogLeNet(**kwargs)\n",
    "        state_dict = load_state_dict_from_url(model_urls['googlenet'],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "        if not original_aux_logits:\n",
    "            model.aux_logits = False\n",
    "            model.aux1 = None\n",
    "            model.aux2 = None\n",
    "        return model\n",
    "\n",
    "    return Network(**kwargs)\n",
    "\n",
    "\n",
    "class NetworkBN(nn.Module):\n",
    "    __constants__ = ['aux_logits', 'transform_input']\n",
    "\n",
    "    def __init__(self, num_classes=3, aux_logits=False, transform_input=False, init_weights=None,\n",
    "                 blocks=None):\n",
    "        super(NetworkBN, self).__init__()\n",
    "        if blocks is None:\n",
    "            blocks = [BasicConv2d, Inception, InceptionAux]\n",
    "        if init_weights is None:\n",
    "            warnings.warn('The default weight initialization of GoogleNet will be changed in future releases of '\n",
    "                          'torchvision. If you wish to keep the old behavior (which leads to long initialization times'\n",
    "                          ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
    "            init_weights = True\n",
    "        assert len(blocks) == 3\n",
    "        conv_block = blocks[0]\n",
    "        inception_block = blocks[1]\n",
    "        inception_aux_block = blocks[2]\n",
    "\n",
    "        self.aux_logits = aux_logits\n",
    "        self.transform_input = transform_input\n",
    "\n",
    "        self.conv1 = conv_block(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.lrn = nn.LocalResponseNorm(5, alpha = 9.9999997e-05)\n",
    "        self.conv2 = conv_block(64, 64, kernel_size=1)\n",
    "        self.lrn2 = nn.LocalResponseNorm(5, alpha = 9.9999997e-05)\n",
    "        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception3a = inception_block(64, 8, 16, 28)\n",
    "        \n",
    "        if aux_logits:\n",
    "            self.aux1 = inception_aux_block(512, num_classes)\n",
    "            self.aux2 = inception_aux_block(528, num_classes)\n",
    "        else:\n",
    "            self.aux1 = None\n",
    "            self.aux2 = None\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((4))\n",
    "        self.dropout = nn.Dropout(0.40000001)\n",
    "        self.fc = nn.Linear(384, num_classes)\n",
    "        #self.softmax = nn.LogSoftmax()\n",
    "\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                X = stats.truncnorm(-2, 2, scale=0.01)\n",
    "                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
    "                values = values.view(m.weight.size())\n",
    "                with torch.no_grad():\n",
    "                    m.weight.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _transform_input(self, x):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        if self.transform_input:\n",
    "            x_ch0 = torch.unsqueeze(x[:, 0].clone(), 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "            x_ch1 = torch.unsqueeze(x[:, 1].clone(), 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "            x_ch2 = torch.unsqueeze(x[:, 2].clone(), 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
    "        return x\n",
    "\n",
    "    def _forward(self, x):\n",
    "        # type: (Tensor) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]\n",
    "        # N x 3 x 224 x 224\n",
    "        # already in conv block\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        x = self.conv1(x)\n",
    "        # N x 64 x 112 x 112\n",
    "        x = F.max_pool2d(x, kernel_size = 3, stride = 2, ceil_mode=True) \n",
    "        x = self.lrn(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.conv2(x)\n",
    "        aux1 = torch.jit.annotate(Optional[Tensor], None)\n",
    "        if self.aux1 is not None:\n",
    "            if self.training:\n",
    "                aux1 = self.aux1(x)\n",
    "        x = self.lrn2(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # N x 64 x 28 x 28\n",
    "        x = self.inception3a(x)\n",
    "        # N x 256 x 28 x 28\n",
    "        \n",
    "        aux2 = torch.jit.annotate(Optional[Tensor], None)\n",
    "        if self.aux2 is not None:\n",
    "            if self.training:\n",
    "                aux2 = self.aux2(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        # N x 1024 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 1024\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        #x = self.softmax(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        #F.log_softmax(x)\n",
    "        return x, aux2, aux1\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def eager_outputs(self, x, aux2, aux1):\n",
    "        # type: (Tensor, Optional[Tensor], Optional[Tensor]) -> GoogLeNetOutputs\n",
    "        if self.training and self.aux_logits:\n",
    "            return _GoogLeNetOutputs(x, aux2, aux1)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # type: (Tensor) -> GoogLeNetOutputs\n",
    "        x = self._transform_input(x)\n",
    "        x, aux1, aux2 = self._forward(x)\n",
    "        aux_defined = self.training and self.aux_logits\n",
    "        if torch.jit.is_scripting():\n",
    "            if not aux_defined:\n",
    "                warnings.warn(\"Scripted GoogleNet always returns GoogleNetOutputs Tuple\")\n",
    "            return GoogLeNetOutputs(x, aux2, aux1)\n",
    "        else:\n",
    "            return self.eager_outputs(x, aux2, aux1)\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, pool_proj,\n",
    "                 conv_block=None):\n",
    "        super(Inception, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "            \n",
    "        self.branch1 = conv_block(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = conv_block(in_channels, ch3x3red, kernel_size=1)\n",
    "\n",
    "\n",
    "    def _forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "\n",
    "\n",
    "        outputs = [branch1, branch2]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, conv_block=None):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.conv = conv_block(in_channels, 128, kernel_size=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14\n",
    "        x = F.adaptive_avg_pool2d(x, (4, 4))\n",
    "        # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4\n",
    "        x = self.conv(x)\n",
    "        # N x 128 x 4 x 4\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 2048\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        # N x 1024\n",
    "        x = F.dropout(x, 0.7, training=self.training)\n",
    "        # N x 1024\n",
    "        x = self.fc2(x)\n",
    "        # N x 1000 (num_classes)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x, inplace=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU's available : 1\n",
      "GPU device name : GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_cuda = True\n",
    "    print(f\"Number of GPU's available : {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU device name : {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU instead\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-9bea0d24b132>:61: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn('The default weight initialization of GoogleNet will be changed in future releases of '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [128, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2        [128, 64, 112, 112]             128\n",
      "       BasicConv2d-3        [128, 64, 112, 112]               0\n",
      " LocalResponseNorm-4          [128, 64, 56, 56]               0\n",
      "            Conv2d-5          [128, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6          [128, 64, 56, 56]             128\n",
      "       BasicConv2d-7          [128, 64, 56, 56]               0\n",
      " LocalResponseNorm-8          [128, 64, 56, 56]               0\n",
      "         MaxPool2d-9          [128, 64, 28, 28]               0\n",
      "           Conv2d-10           [128, 8, 28, 28]             512\n",
      "      BatchNorm2d-11           [128, 8, 28, 28]              16\n",
      "      BasicConv2d-12           [128, 8, 28, 28]               0\n",
      "           Conv2d-13          [128, 16, 28, 28]           1,024\n",
      "      BatchNorm2d-14          [128, 16, 28, 28]              32\n",
      "      BasicConv2d-15          [128, 16, 28, 28]               0\n",
      "        Inception-16          [128, 24, 28, 28]               0\n",
      "AdaptiveAvgPool2d-17            [128, 24, 4, 4]               0\n",
      "          Dropout-18                 [128, 384]               0\n",
      "           Linear-19                   [128, 3]           1,155\n",
      "================================================================\n",
      "Total params: 16,499\n",
      "Trainable params: 16,499\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 73.50\n",
      "Forward/backward pass size (MB): 3455.25\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 3528.82\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = NetworkBN().to(device)\n",
    "summary(model, input_size=(3, 224, 224), batch_size=128, device = str(torch.device(\"cuda\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetworkBN(\n",
       "  (conv1): BasicConv2d(\n",
       "    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (lrn): LocalResponseNorm(5, alpha=9.9999997e-05, beta=0.75, k=1.0)\n",
       "  (conv2): BasicConv2d(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (lrn2): LocalResponseNorm(5, alpha=9.9999997e-05, beta=0.75, k=1.0)\n",
       "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (inception3a): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=4)\n",
       "  (dropout): Dropout(p=0.40000001, inplace=False)\n",
       "  (fc): Linear(in_features=384, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['GoogLeNet', 'googlenet', \"GoogLeNetOutputs\", \"_GoogLeNetOutputs\"]\n",
    "\n",
    "model_urls = {\n",
    "    # GoogLeNet ported from TensorFlow\n",
    "    'googlenet': 'https://download.pytorch.org/models/googlenet-1378be20.pth',\n",
    "}\n",
    "\n",
    "GoogLeNetOutputs = namedtuple('GoogLeNetOutputs', ['logits', 'aux_logits2', 'aux_logits1'])\n",
    "GoogLeNetOutputs.__annotations__ = {'logits': Tensor, 'aux_logits2': Optional[Tensor],\n",
    "                                    'aux_logits1': Optional[Tensor]}\n",
    "\n",
    "# Script annotations failed with _GoogleNetOutputs = namedtuple ...\n",
    "# _GoogLeNetOutputs set here for backwards compat\n",
    "_GoogLeNetOutputs = GoogLeNetOutputs\n",
    "\n",
    "def googlenet(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"GoogLeNet (Inception v1) model architecture from\n",
    "    `\"Going Deeper with Convolutions\" <http://arxiv.org/abs/1409.4842>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        aux_logits (bool): If True, adds two auxiliary branches that can improve training.\n",
    "            Default: *False* when pretrained is True otherwise *True*\n",
    "        transform_input (bool): If True, preprocesses the input according to the method with which it\n",
    "            was trained on ImageNet. Default: *False*\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        if 'transform_input' not in kwargs:\n",
    "            kwargs['transform_input'] = True\n",
    "        if 'aux_logits' not in kwargs:\n",
    "            kwargs['aux_logits'] = False\n",
    "        if kwargs['aux_logits']:\n",
    "            warnings.warn('auxiliary heads in the pretrained googlenet model are NOT pretrained, '\n",
    "                          'so make sure to train them')\n",
    "        original_aux_logits = kwargs['aux_logits']\n",
    "        kwargs['aux_logits'] = False\n",
    "        kwargs['init_weights'] = False\n",
    "        model = GoogLeNet(**kwargs)\n",
    "        state_dict = load_state_dict_from_url(model_urls['googlenet'],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "        if not original_aux_logits:\n",
    "            model.aux_logits = False\n",
    "            model.aux1 = None\n",
    "            model.aux2 = None\n",
    "        return model\n",
    "\n",
    "    return Network(**kwargs)\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    __constants__ = ['aux_logits', 'transform_input']\n",
    "\n",
    "    def __init__(self, num_classes=3, aux_logits=False, transform_input=False, init_weights=None,\n",
    "                 blocks=None):\n",
    "        super(Network, self).__init__()\n",
    "        if blocks is None:\n",
    "            blocks = [BasicConv2d, Inception, InceptionAux]\n",
    "        if init_weights is None:\n",
    "            warnings.warn('The default weight initialization of GoogleNet will be changed in future releases of '\n",
    "                          'torchvision. If you wish to keep the old behavior (which leads to long initialization times'\n",
    "                          ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
    "            init_weights = True\n",
    "        assert len(blocks) == 3\n",
    "        conv_block = blocks[0]\n",
    "        inception_block = blocks[1]\n",
    "        inception_aux_block = blocks[2]\n",
    "\n",
    "        self.aux_logits = aux_logits\n",
    "        self.transform_input = transform_input\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.lrn = nn.LocalResponseNorm(5, alpha = 9.9999997e-05)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=1)\n",
    "        self.lrn2 = nn.LocalResponseNorm(5, alpha = 9.9999997e-05)\n",
    "        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception3a = inception_block(64, 8, 16, 28)\n",
    "        \n",
    "        if aux_logits:\n",
    "            self.aux1 = inception_aux_block(512, num_classes)\n",
    "            self.aux2 = inception_aux_block(528, num_classes)\n",
    "        else:\n",
    "            self.aux1 = None\n",
    "            self.aux2 = None\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((4))\n",
    "        self.dropout = nn.Dropout(0.40000001)\n",
    "        #2048\n",
    "        self.fc = nn.Linear(384, num_classes)\n",
    "        #self.softmax = nn.LogSoftmax()\n",
    "\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                X = stats.truncnorm(-2, 2, scale=0.01)\n",
    "                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
    "                values = values.view(m.weight.size())\n",
    "                with torch.no_grad():\n",
    "                    m.weight.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _transform_input(self, x):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        if self.transform_input:\n",
    "            x_ch0 = torch.unsqueeze(x[:, 0].clone(), 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "            x_ch1 = torch.unsqueeze(x[:, 1].clone(), 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "            x_ch2 = torch.unsqueeze(x[:, 2].clone(), 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
    "        return x\n",
    "\n",
    "    def _forward(self, x):\n",
    "        # type: (Tensor) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]\n",
    "        # N x 3 x 224 x 224\n",
    "        # already in conv block\n",
    "        x = F.relu(self.conv1(x), inplace = True)\n",
    "        #x = self.conv1(x)\n",
    "        # N x 64 x 112 x 112\n",
    "        x = F.max_pool2d(x, kernel_size = 3, stride = 2, ceil_mode=True) \n",
    "        x = self.lrn(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = F.relu(self.conv2(x), inplace = True)\n",
    "        aux1 = torch.jit.annotate(Optional[Tensor], None)\n",
    "        if self.aux1 is not None:\n",
    "            if self.training:\n",
    "                aux1 = self.aux1(x)\n",
    "        x = self.lrn2(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # N x 64 x 28 x 28\n",
    "        x = self.inception3a(x)\n",
    "        # N x 256 x 28 x 28\n",
    "        \n",
    "        aux2 = torch.jit.annotate(Optional[Tensor], None)\n",
    "        if self.aux2 is not None:\n",
    "            if self.training:\n",
    "                aux2 = self.aux2(x)\n",
    "        # N x 256 x 28 x 28\n",
    "        x = self.avgpool(x)\n",
    "        # N x 1024 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 1024\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        #x = self.softmax(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        return x, aux2, aux1\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def eager_outputs(self, x, aux2, aux1):\n",
    "        # type: (Tensor, Optional[Tensor], Optional[Tensor]) -> GoogLeNetOutputs\n",
    "        if self.training and self.aux_logits:\n",
    "            return _GoogLeNetOutputs(x, aux2, aux1)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # type: (Tensor) -> GoogLeNetOutputs\n",
    "        x = self._transform_input(x)\n",
    "        x, aux1, aux2 = self._forward(x)\n",
    "        aux_defined = self.training and self.aux_logits\n",
    "        if torch.jit.is_scripting():\n",
    "            if not aux_defined:\n",
    "                warnings.warn(\"Scripted GoogleNet always returns GoogleNetOutputs Tuple\")\n",
    "            return GoogLeNetOutputs(x, aux2, aux1)\n",
    "        else:\n",
    "            return self.eager_outputs(x, aux2, aux1)\n",
    "\n",
    "class Inception(nn.Module):\n",
    "\n",
    "   # def __init__(self, in_channels, ch1x1, ch3x3red, pool_proj):\n",
    "   #    super(Inception, self).__init__()\n",
    "        \n",
    "   #    self.branch1 = nn.ReLU(nn.Conv2d(in_channels, ch1x1, kernel_size=1))\n",
    "\n",
    "   #    self.branch2 = nn.ReLU(nn.Conv2d(in_channels, ch3x3red, kernel_size=1))\n",
    "\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, pool_proj,\n",
    "                 conv_block=None):\n",
    "        super(Inception, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "            \n",
    "        self.branch1 = conv_block(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = conv_block(in_channels, ch3x3red, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "\n",
    "\n",
    "        outputs = [branch1, branch2]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, conv_block=None):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.conv = conv_block(in_channels, 128, kernel_size=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14\n",
    "        x = F.adaptive_avg_pool2d(x, (4, 4))\n",
    "        # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4\n",
    "        x = self.conv(x)\n",
    "        # N x 128 x 4 x 4\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 2048\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        # N x 1024\n",
    "        x = F.dropout(x, 0.7, training=self.training)\n",
    "        # N x 1024\n",
    "        x = self.fc2(x)\n",
    "        # N x 1000 (num_classes)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x, inplace=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-0bc76ad473e5>:60: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn('The default weight initialization of GoogleNet will be changed in future releases of '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (lrn): LocalResponseNorm(5, alpha=9.9999997e-05, beta=0.75, k=1.0)\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (lrn2): LocalResponseNorm(5, alpha=9.9999997e-05, beta=0.75, k=1.0)\n",
       "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (inception3a): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=4)\n",
       "  (dropout): Dropout(p=0.40000001, inplace=False)\n",
       "  (fc): Linear(in_features=384, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "network = Network()\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [128, 64, 112, 112]           9,472\n",
      " LocalResponseNorm-2          [128, 64, 56, 56]               0\n",
      "            Conv2d-3          [128, 64, 56, 56]           4,160\n",
      " LocalResponseNorm-4          [128, 64, 56, 56]               0\n",
      "         MaxPool2d-5          [128, 64, 28, 28]               0\n",
      "            Conv2d-6           [128, 8, 28, 28]             512\n",
      "       BatchNorm2d-7           [128, 8, 28, 28]              16\n",
      "       BasicConv2d-8           [128, 8, 28, 28]               0\n",
      "            Conv2d-9          [128, 16, 28, 28]           1,024\n",
      "      BatchNorm2d-10          [128, 16, 28, 28]              32\n",
      "      BasicConv2d-11          [128, 16, 28, 28]               0\n",
      "        Inception-12          [128, 24, 28, 28]               0\n",
      "AdaptiveAvgPool2d-13            [128, 24, 4, 4]               0\n",
      "          Dropout-14                 [128, 384]               0\n",
      "           Linear-15                   [128, 3]           1,155\n",
      "================================================================\n",
      "Total params: 16,371\n",
      "Trainable params: 16,371\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 73.50\n",
      "Forward/backward pass size (MB): 1495.25\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 1568.82\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-0bc76ad473e5>:60: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn('The default weight initialization of GoogleNet will be changed in future releases of '\n"
     ]
    }
   ],
   "source": [
    "model = Network().to(device)\n",
    "summary(model, input_size=(3, 224, 224), batch_size=128, device = str(torch.device(\"cuda\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "      return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the hyper-parameters and return a Run namedtuple containing all the \n",
    "# combinations of hyper-parameters\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "        \n",
    "        Run = namedtuple(\"Run\", params.keys())\n",
    "        \n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "            \n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class, help track loss, accuracy, epoch time, run time, \n",
    "# hyper-parameters etc. Also record to TensorBoard and write into csv, json!@!\n",
    "class RunManager():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.epoch_count = 0\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "        self.epoch_start_time = None\n",
    "        \n",
    "        # tracking every run count, run data, hyper-params used, time\n",
    "        self.run_params = None\n",
    "        self.run_count = 0\n",
    "        self.run_data = []\n",
    "        self.run_start_time = None\n",
    "        \n",
    "        self.network = None\n",
    "        self.loader = None\n",
    "        self.tb = None\n",
    "    \n",
    "    def begin_run(self, run, network, loader):\n",
    "        \n",
    "        self.run_start_time = time.time()\n",
    "        \n",
    "        self.run_params = run\n",
    "        self.run_count += 1\n",
    "        \n",
    "        self.network = network\n",
    "        self.loader = loader\n",
    "        self.tb = SummaryWriter(comment = f'-{run}')\n",
    "        \n",
    "        images, labels = next(iter(self.loader))\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "        \n",
    "        #self.tb.add_image('images', grid)\n",
    "        #self.tb.add_graph(self.network, images)\n",
    "        \n",
    "    def end_run(self, model):\n",
    "        \n",
    "        MODEL_PATH = \"./weights/model_{}.pt\".format(self)\n",
    "        torch.save(model, MODEL_PATH)\n",
    "        \n",
    "        self.tb.close()\n",
    "        self.epoch_count = 0\n",
    "       \n",
    "       # zero epoch count, loss, accuracy, \n",
    "    def begin_epoch(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "        self.epoch_count += 1\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "        \n",
    "    # \n",
    "    def end_epoch(self, model):\n",
    "        # calculate epoch duration and run duration(accumulate)\n",
    "        epoch_duration = time.time() - self.epoch_start_time\n",
    "        run_duration = time.time() - self.run_start_time\n",
    "\n",
    "        # record epoch loss and accuracy\n",
    "        loss = self.epoch_loss / len(self.loader.dataset)\n",
    "        accuracy = self.epoch_num_correct / len(self.loader.dataset)\n",
    "\n",
    "        # Record epoch loss and accuracy to TensorBoard \n",
    "        self.tb.add_scalar('Loss', loss, self.epoch_count)\n",
    "        self.tb.add_scalar('Accuracy', accuracy, self.epoch_count)\n",
    "\n",
    "        # Record params to TensorBoard\n",
    "        for name, param in self.network.named_parameters():\n",
    "            self.tb.add_histogram(name, param, self.epoch_count)\n",
    "            self.tb.add_histogram(f'{name}.grad', param.grad, self.epoch_count)\n",
    "    \n",
    "\n",
    "        # Write into 'results' (OrderedDict) for all run related data\n",
    "        results = OrderedDict()\n",
    "        results[\"run\"] = self.run_count\n",
    "        results[\"epoch\"] = self.epoch_count\n",
    "        results[\"loss\"] = loss\n",
    "        results[\"accuracy\"] = accuracy\n",
    "        results[\"epoch duration\"] = epoch_duration\n",
    "        results[\"run duration\"] = run_duration\n",
    "        \n",
    "        MODEL_PATH = \"./weights/model_{}.pt\".format(self.epoch_count)\n",
    "        torch.save(model, MODEL_PATH)\n",
    "        \n",
    "        # Record hyper-params into 'results'\n",
    "        for k,v in self.run_params._asdict().items(): results[k] = v\n",
    "        self.run_data.append(results)\n",
    "        df = pd.DataFrame.from_dict(self.run_data, orient = 'columns')\n",
    "\n",
    "        # display epoch information and show progress\n",
    "        clear_output(wait=True)\n",
    "        display(df)\n",
    "\n",
    "      # accumulate loss of batch into entire epoch loss\n",
    "    def track_loss(self, loss):\n",
    "        # multiply batch size so variety of batch sizes can be compared\n",
    "        self.epoch_loss += loss.item() * self.loader.batch_size\n",
    "\n",
    "      # accumulate number of corrects of batch into entire epoch num_correct\n",
    "    def track_num_correct(self, preds, labels):\n",
    "        self.epoch_num_correct += self._get_num_correct(preds, labels)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _get_num_correct(self, preds, lables):\n",
    "        return preds.argmax(dim = 1).eq(labels).sum().item()\n",
    "    \n",
    "    def save(self, fileName):\n",
    "        \n",
    "        pd.DataFrame.from_dict(\n",
    "        self.run_data,\n",
    "        orient = \"columns\").to_csv(f'{fileName}.csv')\n",
    "        \n",
    "        \n",
    "        \n",
    "        with open(f'{fileName}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.run_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.ImageFolder(\n",
    "    root = './Data_cleaned/train'\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 263 ms, sys: 310 ms, total: 573 ms\n",
      "Wall time: 2.09 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.4738), tensor(0.2598))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size = 512, num_workers = 2, pin_memory = True\n",
    ")\n",
    "data = next(iter(loader))\n",
    "mean = data[0].mean()\n",
    "std = data[0].std()\n",
    "mean, std\n",
    "\n",
    "#nb_samples = 0.\n",
    "#channel_mean = torch.Tensor([0., 0., 0.])\n",
    "#channel_std = torch.Tensor([0., 0., 0.])\n",
    "#for images in tqdm_notebook(loader):\n",
    "    # scale image to be between 0 and 1 \n",
    "    \n",
    "#    images = np.asarray(images)/np.asarray(255.)\n",
    "#    batch_samples = images.size(0)\n",
    "#    images = images.view(batch_samples, images.size(1)*images.size(2), 3)\n",
    "#    for i in range(3):\n",
    "#        channel_mean[i]+=images[:, :,i].mean(1).sum(0)\n",
    "#        channel_std[i]+=images[:, :,i].std(1).sum(0)\n",
    "#    nb_samples += batch_samples\n",
    "\n",
    "#channel_mean /= nb_samples\n",
    "#channel_std /= nb_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_normal = torchvision.datasets.ImageFolder(\n",
    "    root = './Data_cleaned/train'\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsets = {\n",
    "    'not_normal': train_set,\n",
    "    'normal' : train_set_normal\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-9bea0d24b132>:61: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn('The default weight initialization of GoogleNet will be changed in future releases of '\n"
     ]
    }
   ],
   "source": [
    "networkBN = NetworkBN()\n",
    "#network2 = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = {\n",
    "#    'no_batch_norm': network,\n",
    "    'batch_norm': networkBN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIR = '/home/vadim/testovoe/dev/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(CURRENT_DIR+\"Data/text.txt\", sep=\"\\s+\", header=None, names=[\"name\", \"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_width(im, new_shape, is_rgb=True):\n",
    "    pad_diff = new_shape - im.shape[0], new_shape - im.shape[1]\n",
    "    t, b = math.floor(pad_diff[0]/2), math.ceil(pad_diff[0]/2)\n",
    "    l, r = math.floor(pad_diff[1]/2), math.ceil(pad_diff[1]/2)\n",
    "    if is_rgb:\n",
    "        pad_width = ((t,b), (l,r), (0, 0))\n",
    "    else:\n",
    "        pad_width = ((t,b), (l,r))\n",
    "    return pad_width\n",
    "\n",
    "def preprocess_image(image_path, desired_size=224):\n",
    "    im = Image.open(image_path)\n",
    "    im = im.resize((desired_size, )*2, resample=Image.LANCZOS)\n",
    "    \n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:00<00:00, 1071.24it/s]\n"
     ]
    }
   ],
   "source": [
    "N = test_df.shape[0]\n",
    "x_test = np.empty((N, 224, 224, 3), dtype=np.uint8)\n",
    "\n",
    "for i, image_id in enumerate(tqdm(test_df['name'])):\n",
    "    x_test[i, :, :, :] = preprocess_image(\n",
    "         f'/home/vadim/testovoe/dev/{image_id}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.get_dummies(test_df['category']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>num_workers</th>\n",
       "      <th>device</th>\n",
       "      <th>trainset</th>\n",
       "      <th>network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.749283</td>\n",
       "      <td>0.762316</td>\n",
       "      <td>55.870668</td>\n",
       "      <td>56.533402</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.874722</td>\n",
       "      <td>0.681649</td>\n",
       "      <td>57.136831</td>\n",
       "      <td>113.748542</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.872166</td>\n",
       "      <td>0.679366</td>\n",
       "      <td>57.244528</td>\n",
       "      <td>171.064068</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.849633</td>\n",
       "      <td>0.677885</td>\n",
       "      <td>56.928435</td>\n",
       "      <td>228.064021</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.797469</td>\n",
       "      <td>0.694790</td>\n",
       "      <td>57.801470</td>\n",
       "      <td>285.946223</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>0.019135</td>\n",
       "      <td>0.995928</td>\n",
       "      <td>68.799151</td>\n",
       "      <td>5907.251275</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>0.018024</td>\n",
       "      <td>0.996082</td>\n",
       "      <td>68.771111</td>\n",
       "      <td>5976.116111</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>0.017682</td>\n",
       "      <td>0.995681</td>\n",
       "      <td>69.345453</td>\n",
       "      <td>6045.554350</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>0.016845</td>\n",
       "      <td>0.996021</td>\n",
       "      <td>64.861036</td>\n",
       "      <td>6110.502424</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.014834</td>\n",
       "      <td>0.997039</td>\n",
       "      <td>66.258320</td>\n",
       "      <td>6176.852132</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    run  epoch      loss  accuracy  epoch duration  run duration      lr  \\\n",
       "0     1      1  0.749283  0.762316       55.870668     56.533402  0.0003   \n",
       "1     1      2  0.874722  0.681649       57.136831    113.748542  0.0003   \n",
       "2     1      3  0.872166  0.679366       57.244528    171.064068  0.0003   \n",
       "3     1      4  0.849633  0.677885       56.928435    228.064021  0.0003   \n",
       "4     1      5  0.797469  0.694790       57.801470    285.946223  0.0003   \n",
       "..  ...    ...       ...       ...             ...           ...     ...   \n",
       "95    1     96  0.019135  0.995928       68.799151   5907.251275  0.0003   \n",
       "96    1     97  0.018024  0.996082       68.771111   5976.116111  0.0003   \n",
       "97    1     98  0.017682  0.995681       69.345453   6045.554350  0.0003   \n",
       "98    1     99  0.016845  0.996021       64.861036   6110.502424  0.0003   \n",
       "99    1    100  0.014834  0.997039       66.258320   6176.852132  0.0003   \n",
       "\n",
       "    batch_size  weight_decay  num_workers device trainset     network  \n",
       "0          128      0.000003            1   cuda   normal  batch_norm  \n",
       "1          128      0.000003            1   cuda   normal  batch_norm  \n",
       "2          128      0.000003            1   cuda   normal  batch_norm  \n",
       "3          128      0.000003            1   cuda   normal  batch_norm  \n",
       "4          128      0.000003            1   cuda   normal  batch_norm  \n",
       "..         ...           ...          ...    ...      ...         ...  \n",
       "95         128      0.000003            1   cuda   normal  batch_norm  \n",
       "96         128      0.000003            1   cuda   normal  batch_norm  \n",
       "97         128      0.000003            1   cuda   normal  batch_norm  \n",
       "98         128      0.000003            1   cuda   normal  batch_norm  \n",
       "99         128      0.000003            1   cuda   normal  batch_norm  \n",
       "\n",
       "[100 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44min 23s, sys: 5min 47s, total: 50min 10s\n",
      "Wall time: 1h 42min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test different configurations\n",
    "# for every run [value] that is going to be used e.g [.001, .01] = two runs\n",
    "# the bigger the batch the less gradient updates steps made\n",
    "params = OrderedDict(\n",
    "    lr = [.0003],\n",
    "    # max batch size 352\n",
    "    batch_size = [128],\n",
    "    weight_decay = [3.0000001e-06],\n",
    "    num_workers = [1],\n",
    "    device = [\"cuda\"],\n",
    "    trainset = [\"normal\"],\n",
    "    # try all the values in the dict network1, network2\n",
    "    network = list(networks.keys())\n",
    ")\n",
    "m = RunManager()\n",
    "# active run or current run\n",
    "# this thing detecting RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output. \n",
    "# if k = 0 in LRN layers\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for run in RunBuilder.get_runs(params):\n",
    "    \n",
    "    device = torch.device(run.device)\n",
    "    # redefine the network\n",
    "    network = networks[run.network].to(device)\n",
    "    network.train()\n",
    "    loader = DataLoader(trainsets[run.trainset], batch_size = run.batch_size, num_workers = run.num_workers, \n",
    "                       pin_memory = True) \n",
    "    optimizer = optim.Adam(network.parameters(), lr = run.lr, weight_decay = run.weight_decay) \n",
    "    \n",
    "    m.begin_run(run, network, loader)\n",
    "    for epoch in range(100):\n",
    "        m.begin_epoch()\n",
    "        for batch in loader:\n",
    "            #network.train()\n",
    "            images = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            preds = network(images)\n",
    "            loss = F.cross_entropy(preds, labels)\n",
    "            #7.9\n",
    "            #optimizer.zero_grad()\n",
    "            #8 sec\n",
    "            for p in network.parameters(): p.grad = None\n",
    "            loss.backward() # Calculate gradients\n",
    "            optimizer.step() # Update Weights\n",
    "            m.track_loss(loss)\n",
    "            m.track_num_correct(preds, labels)\n",
    "        m.end_epoch(network)\n",
    "    m.end_run(network)\n",
    "m.save(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./weights/model_99.pt\"\n",
    "#model = NetworkBN()\n",
    "model = torch.load(MODEL_PATH)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>num_workers</th>\n",
       "      <th>device</th>\n",
       "      <th>trainset</th>\n",
       "      <th>network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.014834</td>\n",
       "      <td>0.997039</td>\n",
       "      <td>66.258320</td>\n",
       "      <td>6176.852132</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>0.018024</td>\n",
       "      <td>0.996082</td>\n",
       "      <td>68.771111</td>\n",
       "      <td>5976.116111</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>0.016845</td>\n",
       "      <td>0.996021</td>\n",
       "      <td>64.861036</td>\n",
       "      <td>6110.502424</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>0.019135</td>\n",
       "      <td>0.995928</td>\n",
       "      <td>68.799151</td>\n",
       "      <td>5907.251275</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>0.019787</td>\n",
       "      <td>0.995774</td>\n",
       "      <td>69.460165</td>\n",
       "      <td>5838.355963</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.737526</td>\n",
       "      <td>0.724774</td>\n",
       "      <td>58.075132</td>\n",
       "      <td>344.097553</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.797469</td>\n",
       "      <td>0.694790</td>\n",
       "      <td>57.801470</td>\n",
       "      <td>285.946223</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.874722</td>\n",
       "      <td>0.681649</td>\n",
       "      <td>57.136831</td>\n",
       "      <td>113.748542</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.872166</td>\n",
       "      <td>0.679366</td>\n",
       "      <td>57.244528</td>\n",
       "      <td>171.064068</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.849633</td>\n",
       "      <td>0.677885</td>\n",
       "      <td>56.928435</td>\n",
       "      <td>228.064021</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    run  epoch      loss  accuracy  epoch duration  run duration      lr  \\\n",
       "99    1    100  0.014834  0.997039       66.258320   6176.852132  0.0003   \n",
       "96    1     97  0.018024  0.996082       68.771111   5976.116111  0.0003   \n",
       "98    1     99  0.016845  0.996021       64.861036   6110.502424  0.0003   \n",
       "95    1     96  0.019135  0.995928       68.799151   5907.251275  0.0003   \n",
       "94    1     95  0.019787  0.995774       69.460165   5838.355963  0.0003   \n",
       "..  ...    ...       ...       ...             ...           ...     ...   \n",
       "5     1      6  0.737526  0.724774       58.075132    344.097553  0.0003   \n",
       "4     1      5  0.797469  0.694790       57.801470    285.946223  0.0003   \n",
       "1     1      2  0.874722  0.681649       57.136831    113.748542  0.0003   \n",
       "2     1      3  0.872166  0.679366       57.244528    171.064068  0.0003   \n",
       "3     1      4  0.849633  0.677885       56.928435    228.064021  0.0003   \n",
       "\n",
       "    batch_size  weight_decay  num_workers device trainset     network  \n",
       "99         128      0.000003            1   cuda   normal  batch_norm  \n",
       "96         128      0.000003            1   cuda   normal  batch_norm  \n",
       "98         128      0.000003            1   cuda   normal  batch_norm  \n",
       "95         128      0.000003            1   cuda   normal  batch_norm  \n",
       "94         128      0.000003            1   cuda   normal  batch_norm  \n",
       "..         ...           ...          ...    ...      ...         ...  \n",
       "5          128      0.000003            1   cuda   normal  batch_norm  \n",
       "4          128      0.000003            1   cuda   normal  batch_norm  \n",
       "1          128      0.000003            1   cuda   normal  batch_norm  \n",
       "2          128      0.000003            1   cuda   normal  batch_norm  \n",
       "3          128      0.000003            1   cuda   normal  batch_norm  \n",
       "\n",
       "[100 rows x 13 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(m.run_data).sort_values(\"accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=[x.strip() for x in open('labels_adam.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 359.80 fps\n",
      "CPU times: user 2.76 s, sys: 20.7 ms, total: 2.78 s\n",
      "Wall time: 2.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This could be done by defining separate dataloader for test dataset\n",
    "preds_adam = []\n",
    "preds = []\n",
    "from timeit import default_timer as timer\n",
    "time_start = timer()\n",
    "device = torch.device(\"cuda\")\n",
    "# this is untrained\n",
    "# uncomment for 67% accu\n",
    "#model = networks['batch_norm'].to(device)\n",
    "#model = Network().to(device)\n",
    "#model = model.to(device)\n",
    "def get_preds(network):\n",
    "        network = network\n",
    "        # this is very ineffective O(n^2)+complexity of network\n",
    "        # model.eval() will notify all your layers that you are in eval mode, that way, \n",
    "        # batchnorm or dropout layers will work in eval mode instead of training mode.\n",
    "        # torch.no_grad() impacts the autograd engine and deactivate it. \n",
    "        # It will reduce memory usage and speed up computations but you won’t be able to backprop \n",
    "        # (which you don’t want in an eval script).\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, x in enumerate(categories):\n",
    "                for j, y in enumerate(sorted(glob.glob('Data/test/{}/*'.format(x)))):\n",
    "                    im = Image.open(y)\n",
    "                    image = transforms.ToTensor()(im).unsqueeze_(0).to(device)\n",
    "                    output = model(image) # preds\n",
    "                    preds = output.argmax(dim=1, keepdim=True)\n",
    "                    preds_adam.extend(preds.cpu().numpy())\n",
    "                    #preds_adam.append(preds.detach().cpu().clone().numpy())\n",
    "                    #print(preds_adam)\n",
    "                    #preds_np = pred.detach().cpu().clone().numpy()\n",
    "                    #print(preds_np)\n",
    "                    #preds_np.append(pred.detach().cpu().clone().numpy())\n",
    "                    #preds_adam.extend(pred.cpu().numpy())\n",
    "                    #print(preds)\n",
    "                    #print(preds.argmax(dim = 1))\n",
    "                    #preds_np.append(preds.detach().cpu().clone().numpy())\n",
    "                    #preds_adam.append(np.argmax(preds_np))\n",
    "                    #print(y, categories[np.argmax(pred['softmax'])])\n",
    "        return preds_adam\n",
    "preds_adam = get_preds(network)\n",
    "time_end = timer()\n",
    "fps_adam=('FPS: %.2f fps' % (1000/(time_end-time_start)))\n",
    "print('FPS: %.2f fps' % (1000/(time_end-time_start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 61.68 fps\n",
      "CPU times: user 15 s, sys: 1.07 s, total: 16 s\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This could be done by defining separate dataloader for test dataset\n",
    "preds_adam = []\n",
    "preds = []\n",
    "from timeit import default_timer as timer\n",
    "time_start = timer()\n",
    "device = torch.device(\"cuda\")\n",
    "def predictions():\n",
    "    for i, x in enumerate(categories):\n",
    "        for j, y in enumerate(sorted(glob.glob('Data/test/{}/*'.format(x)))):\n",
    "            im = Image.open(y)\n",
    "            image = transforms.ToTensor()(im).unsqueeze_(0).to(device)\n",
    "            output = model(image) # preds\n",
    "            #print(output)\n",
    "            preds = output.argmax(dim=1, keepdim=True)\n",
    "            #print(categories[np.argmax(output)])\n",
    "            preds_adam.extend(preds.cpu().numpy())\n",
    "            #preds_adam.append(preds.detach().cpu().clone().numpy())\n",
    "            #print(preds_adam)\n",
    "            #preds_np = pred.detach().cpu().clone().numpy()\n",
    "            #print(preds_np)\n",
    "            #preds_np.append(pred.detach().cpu().clone().numpy())\n",
    "            #preds_adam.extend(pred.cpu().numpy())\n",
    "            #print(preds)\n",
    "            #print(preds.argmax(dim = 1))\n",
    "            #preds_np.append(preds.detach().cpu().clone().numpy())\n",
    "            #preds_adam.append(np.argmax(preds_np))\n",
    "            #print(y, categories[np.argmax(pred['softmax'])])\n",
    "    return preds_adam\n",
    "preds_adam = predictions()\n",
    "time_end = timer()\n",
    "fps=('FPS: %.2f fps' % (1000/(time_end-time_start)))\n",
    "print('FPS: %.2f fps' % (1000/(time_end-time_start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of numpy arrays\n",
    "preds_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDigits(lst): \n",
    "    return list(map(lambda el:el.tolist(), lst)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractDigits(preds_adam)\n",
    "preds_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in preds_adam for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = LabelBinarizer()\n",
    "preds_adam = lb.fit_transform(flat_list)\n",
    "preds_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.68      0.72       300\n",
      "           1       0.64      0.42      0.51       300\n",
      "           2       0.55      0.81      0.66       300\n",
      "\n",
      "   micro avg       0.64      0.64      0.64       900\n",
      "   macro avg       0.65      0.64      0.63       900\n",
      "weighted avg       0.65      0.64      0.63       900\n",
      " samples avg       0.64      0.64      0.64       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds_adam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7570370370370371\n"
     ]
    }
   ],
   "source": [
    "mAP_adam = str(average_precision_score(y_test, preds_adam, average=\"samples\"))\n",
    "print(average_precision_score(y_test, preds_adam, average=\"samples\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = torchvision.datasets.ImageFolder(\n",
    "    root = './Data/test'\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_set, batch_size = 352, num_workers = 1, \n",
    "                       pin_memory = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, optimizer, test_loader):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model : network\n",
    "        test_loader : pytorch test loader\n",
    "    Returns:\n",
    "        list : predicted values\n",
    "    \"\"\"\n",
    "    \n",
    "    model = torch.load(MODEL_PATH)\n",
    "    optimizer = optimizer\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            output = model(images)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-9bea0d24b132>:61: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn('The default weight initialization of GoogleNet will be changed in future releases of '\n"
     ]
    }
   ],
   "source": [
    "model = NetworkBN().to(device)\n",
    "optimizer = optim.Adam(network.parameters(), lr = 0.0003)\n",
    "\n",
    "# get predictions\n",
    "predictions = test(torch.load(MODEL_PATH), optimizer, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractDigits(lst): \n",
    "    return list(map(lambda el:el.tolist(), lst)) \n",
    "extractDigits(preds)\n",
    "flat_list = []\n",
    "flat_list = [item for sublist in preds for item in sublist]\n",
    "flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = LabelBinarizer()\n",
    "preds = lb.fit_transform(flat_list)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       300\n",
      "           1       0.80      0.99      0.89       300\n",
      "           2       1.00      0.75      0.86       300\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       900\n",
      "   macro avg       0.93      0.92      0.91       900\n",
      "weighted avg       0.93      0.92      0.91       900\n",
      " samples avg       0.92      0.92      0.92       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9437037037037036\n"
     ]
    }
   ],
   "source": [
    "mAP_adam = str(average_precision_score(y_test, preds, average=\"samples\"))\n",
    "print(average_precision_score(y_test, preds, average=\"samples\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = sum([param.nelement() for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
