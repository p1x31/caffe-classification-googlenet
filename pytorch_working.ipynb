{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from numpy import prod\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "\n",
    "from itertools import product\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "import warnings\n",
    "\"\"\"Optional[...] is a shorthand notation for Union[..., None], \n",
    "telling the type checker that either an object of the specific type is required, \n",
    "or None is required. ... stands for any valid type hint, \n",
    "including complex compound types or a Union[] of more types. \n",
    "Whenever you have a keyword argument with default value None, \n",
    "you should use Optional.\"\"\"\n",
    "from torch.jit.annotations import Optional, Tuple\n",
    "\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['GoogLeNet', 'googlenet', \"GoogLeNetOutputs\", \"_GoogLeNetOutputs\"]\n",
    "\n",
    "model_urls = {\n",
    "    # GoogLeNet ported from TensorFlow\n",
    "    'googlenet': 'https://download.pytorch.org/models/googlenet-1378be20.pth',\n",
    "}\n",
    "\n",
    "GoogLeNetOutputs = namedtuple('GoogLeNetOutputs', ['logits', 'aux_logits2', 'aux_logits1'])\n",
    "GoogLeNetOutputs.__annotations__ = {'logits': Tensor, 'aux_logits2': Optional[Tensor],\n",
    "                                    'aux_logits1': Optional[Tensor]}\n",
    "\n",
    "# Script annotations failed with _GoogleNetOutputs = namedtuple ...\n",
    "# _GoogLeNetOutputs set here for backwards compat\n",
    "_GoogLeNetOutputs = GoogLeNetOutputs\n",
    "\n",
    "\n",
    "def googlenet(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"GoogLeNet (Inception v1) model architecture from\n",
    "    `\"Going Deeper with Convolutions\" <http://arxiv.org/abs/1409.4842>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        aux_logits (bool): If True, adds two auxiliary branches that can improve training.\n",
    "            Default: *False* when pretrained is True otherwise *True*\n",
    "        transform_input (bool): If True, preprocesses the input according to the method with which it\n",
    "            was trained on ImageNet. Default: *False*\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        if 'transform_input' not in kwargs:\n",
    "            kwargs['transform_input'] = True\n",
    "        if 'aux_logits' not in kwargs:\n",
    "            kwargs['aux_logits'] = False\n",
    "        if kwargs['aux_logits']:\n",
    "            warnings.warn('auxiliary heads in the pretrained googlenet model are NOT pretrained, '\n",
    "                          'so make sure to train them')\n",
    "        original_aux_logits = kwargs['aux_logits']\n",
    "        kwargs['aux_logits'] = False\n",
    "        kwargs['init_weights'] = True\n",
    "        model = GoogLeNet(**kwargs)\n",
    "        state_dict = load_state_dict_from_url(model_urls['googlenet'],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "        if not original_aux_logits:\n",
    "            model.aux_logits = False\n",
    "            model.aux1 = None\n",
    "            model.aux2 = None\n",
    "        return model\n",
    "\n",
    "    return Network(**kwargs)\n",
    "\n",
    "\n",
    "class NetworkBN(nn.Module):\n",
    "    __constants__ = ['aux_logits', 'transform_input']\n",
    "\n",
    "    def __init__(self, num_classes=3, aux_logits=False, transform_input=False, init_weights=None,\n",
    "                 blocks=None):\n",
    "        super(NetworkBN, self).__init__()\n",
    "        if blocks is None:\n",
    "            blocks = [BasicConv2d, Inception, InceptionAux]\n",
    "        if init_weights is None:\n",
    "            warnings.warn('The default weight initialization of GoogleNet will be changed in future releases of '\n",
    "                          'torchvision. If you wish to keep the old behavior (which leads to long initialization times'\n",
    "                          ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
    "            init_weights = True\n",
    "        assert len(blocks) == 3\n",
    "        conv_block = blocks[0]\n",
    "        inception_block = blocks[1]\n",
    "        inception_aux_block = blocks[2]\n",
    "\n",
    "        self.aux_logits = aux_logits\n",
    "        self.transform_input = transform_input\n",
    "\n",
    "        self.conv1 = conv_block(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.lrn = nn.LocalResponseNorm(5, alpha = 9.9999997e-05)\n",
    "        self.conv2 = conv_block(64, 64, kernel_size=1)\n",
    "        self.lrn2 = nn.LocalResponseNorm(5, alpha = 9.9999997e-05)\n",
    "        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception3a = inception_block(64, 8, 16, 28)\n",
    "        \n",
    "        if aux_logits:\n",
    "            self.aux1 = inception_aux_block(512, num_classes)\n",
    "            self.aux2 = inception_aux_block(528, num_classes)\n",
    "        else:\n",
    "            self.aux1 = None\n",
    "            self.aux2 = None\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((4))\n",
    "        self.dropout = nn.Dropout(0.40000001)\n",
    "        self.fc = nn.Linear(384, num_classes)\n",
    "        #self.softmax = nn.LogSoftmax()\n",
    "\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                X = stats.truncnorm(-2, 2, scale=0.01)\n",
    "                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
    "                values = values.view(m.weight.size())\n",
    "                with torch.no_grad():\n",
    "                    m.weight.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _transform_input(self, x):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        if self.transform_input:\n",
    "            x_ch0 = torch.unsqueeze(x[:, 0].clone(), 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "            x_ch1 = torch.unsqueeze(x[:, 1].clone(), 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "            x_ch2 = torch.unsqueeze(x[:, 2].clone(), 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
    "        return x\n",
    "\n",
    "    def _forward(self, x):\n",
    "        # type: (Tensor) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]\n",
    "        # N x 3 x 224 x 224\n",
    "        # already in conv block\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        x = self.conv1(x)\n",
    "        # N x 64 x 112 x 112\n",
    "        x = F.max_pool2d(x, kernel_size = 3, stride = 2, ceil_mode=True) \n",
    "        x = self.lrn(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.conv2(x)\n",
    "        aux1 = torch.jit.annotate(Optional[Tensor], None)\n",
    "        if self.aux1 is not None:\n",
    "            if self.training:\n",
    "                aux1 = self.aux1(x)\n",
    "        x = self.lrn2(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # N x 64 x 28 x 28\n",
    "        x = self.inception3a(x)\n",
    "        # N x 256 x 28 x 28\n",
    "        \n",
    "        aux2 = torch.jit.annotate(Optional[Tensor], None)\n",
    "        if self.aux2 is not None:\n",
    "            if self.training:\n",
    "                aux2 = self.aux2(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        # N x 1024 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 1024\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        #x = self.softmax(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        #F.log_softmax(x)\n",
    "        return x, aux2, aux1\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def eager_outputs(self, x, aux2, aux1):\n",
    "        # type: (Tensor, Optional[Tensor], Optional[Tensor]) -> GoogLeNetOutputs\n",
    "        if self.training and self.aux_logits:\n",
    "            return _GoogLeNetOutputs(x, aux2, aux1)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # type: (Tensor) -> GoogLeNetOutputs\n",
    "        x = self._transform_input(x)\n",
    "        x, aux1, aux2 = self._forward(x)\n",
    "        aux_defined = self.training and self.aux_logits\n",
    "        if torch.jit.is_scripting():\n",
    "            if not aux_defined:\n",
    "                warnings.warn(\"Scripted GoogleNet always returns GoogleNetOutputs Tuple\")\n",
    "            return GoogLeNetOutputs(x, aux2, aux1)\n",
    "        else:\n",
    "            return self.eager_outputs(x, aux2, aux1)\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, pool_proj,\n",
    "                 conv_block=None):\n",
    "        super(Inception, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "            \n",
    "        self.branch1 = conv_block(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = conv_block(in_channels, ch3x3red, kernel_size=1)\n",
    "\n",
    "\n",
    "    def _forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "\n",
    "\n",
    "        outputs = [branch1, branch2]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, conv_block=None):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.conv = conv_block(in_channels, 128, kernel_size=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14\n",
    "        x = F.adaptive_avg_pool2d(x, (4, 4))\n",
    "        # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4\n",
    "        x = self.conv(x)\n",
    "        # N x 128 x 4 x 4\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 2048\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        # N x 1024\n",
    "        x = F.dropout(x, 0.7, training=self.training)\n",
    "        # N x 1024\n",
    "        x = self.fc2(x)\n",
    "        # N x 1000 (num_classes)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x, inplace=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU's available : 1\n",
      "GPU device name : GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_cuda = True\n",
    "    print(f\"Number of GPU's available : {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU device name : {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU instead\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-9bea0d24b132>:61: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn('The default weight initialization of GoogleNet will be changed in future releases of '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [128, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2        [128, 64, 112, 112]             128\n",
      "       BasicConv2d-3        [128, 64, 112, 112]               0\n",
      " LocalResponseNorm-4          [128, 64, 56, 56]               0\n",
      "            Conv2d-5          [128, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6          [128, 64, 56, 56]             128\n",
      "       BasicConv2d-7          [128, 64, 56, 56]               0\n",
      " LocalResponseNorm-8          [128, 64, 56, 56]               0\n",
      "         MaxPool2d-9          [128, 64, 28, 28]               0\n",
      "           Conv2d-10           [128, 8, 28, 28]             512\n",
      "      BatchNorm2d-11           [128, 8, 28, 28]              16\n",
      "      BasicConv2d-12           [128, 8, 28, 28]               0\n",
      "           Conv2d-13          [128, 16, 28, 28]           1,024\n",
      "      BatchNorm2d-14          [128, 16, 28, 28]              32\n",
      "      BasicConv2d-15          [128, 16, 28, 28]               0\n",
      "        Inception-16          [128, 24, 28, 28]               0\n",
      "AdaptiveAvgPool2d-17            [128, 24, 4, 4]               0\n",
      "          Dropout-18                 [128, 384]               0\n",
      "           Linear-19                   [128, 3]           1,155\n",
      "================================================================\n",
      "Total params: 16,499\n",
      "Trainable params: 16,499\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 73.50\n",
      "Forward/backward pass size (MB): 3455.25\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 3528.82\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = NetworkBN().to(device)\n",
    "summary(model, input_size=(3, 224, 224), batch_size=128, device = str(torch.device(\"cuda\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetworkBN(\n",
       "  (conv1): BasicConv2d(\n",
       "    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (lrn): LocalResponseNorm(5, alpha=9.9999997e-05, beta=0.75, k=1.0)\n",
       "  (conv2): BasicConv2d(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (lrn2): LocalResponseNorm(5, alpha=9.9999997e-05, beta=0.75, k=1.0)\n",
       "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (inception3a): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=4)\n",
       "  (dropout): Dropout(p=0.40000001, inplace=False)\n",
       "  (fc): Linear(in_features=384, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['GoogLeNet', 'googlenet', \"GoogLeNetOutputs\", \"_GoogLeNetOutputs\"]\n",
    "\n",
    "model_urls = {\n",
    "    # GoogLeNet ported from TensorFlow\n",
    "    'googlenet': 'https://download.pytorch.org/models/googlenet-1378be20.pth',\n",
    "}\n",
    "\n",
    "GoogLeNetOutputs = namedtuple('GoogLeNetOutputs', ['logits', 'aux_logits2', 'aux_logits1'])\n",
    "GoogLeNetOutputs.__annotations__ = {'logits': Tensor, 'aux_logits2': Optional[Tensor],\n",
    "                                    'aux_logits1': Optional[Tensor]}\n",
    "\n",
    "# Script annotations failed with _GoogleNetOutputs = namedtuple ...\n",
    "# _GoogLeNetOutputs set here for backwards compat\n",
    "_GoogLeNetOutputs = GoogLeNetOutputs\n",
    "\n",
    "def googlenet(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"GoogLeNet (Inception v1) model architecture from\n",
    "    `\"Going Deeper with Convolutions\" <http://arxiv.org/abs/1409.4842>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        aux_logits (bool): If True, adds two auxiliary branches that can improve training.\n",
    "            Default: *False* when pretrained is True otherwise *True*\n",
    "        transform_input (bool): If True, preprocesses the input according to the method with which it\n",
    "            was trained on ImageNet. Default: *False*\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        if 'transform_input' not in kwargs:\n",
    "            kwargs['transform_input'] = True\n",
    "        if 'aux_logits' not in kwargs:\n",
    "            kwargs['aux_logits'] = False\n",
    "        if kwargs['aux_logits']:\n",
    "            warnings.warn('auxiliary heads in the pretrained googlenet model are NOT pretrained, '\n",
    "                          'so make sure to train them')\n",
    "        original_aux_logits = kwargs['aux_logits']\n",
    "        kwargs['aux_logits'] = False\n",
    "        kwargs['init_weights'] = False\n",
    "        model = GoogLeNet(**kwargs)\n",
    "        state_dict = load_state_dict_from_url(model_urls['googlenet'],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "        if not original_aux_logits:\n",
    "            model.aux_logits = False\n",
    "            model.aux1 = None\n",
    "            model.aux2 = None\n",
    "        return model\n",
    "\n",
    "    return Network(**kwargs)\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    __constants__ = ['aux_logits', 'transform_input']\n",
    "\n",
    "    def __init__(self, num_classes=3, aux_logits=False, transform_input=False, init_weights=None,\n",
    "                 blocks=None):\n",
    "        super(Network, self).__init__()\n",
    "        if blocks is None:\n",
    "            blocks = [BasicConv2d, Inception, InceptionAux]\n",
    "        if init_weights is None:\n",
    "            warnings.warn('The default weight initialization of GoogleNet will be changed in future releases of '\n",
    "                          'torchvision. If you wish to keep the old behavior (which leads to long initialization times'\n",
    "                          ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
    "            init_weights = True\n",
    "        assert len(blocks) == 3\n",
    "        conv_block = blocks[0]\n",
    "        inception_block = blocks[1]\n",
    "        inception_aux_block = blocks[2]\n",
    "\n",
    "        self.aux_logits = aux_logits\n",
    "        self.transform_input = transform_input\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.lrn = nn.LocalResponseNorm(5, alpha = 9.9999997e-05)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=1)\n",
    "        self.lrn2 = nn.LocalResponseNorm(5, alpha = 9.9999997e-05)\n",
    "        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception3a = inception_block(64, 8, 16, 28)\n",
    "        \n",
    "        if aux_logits:\n",
    "            self.aux1 = inception_aux_block(512, num_classes)\n",
    "            self.aux2 = inception_aux_block(528, num_classes)\n",
    "        else:\n",
    "            self.aux1 = None\n",
    "            self.aux2 = None\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((4))\n",
    "        self.dropout = nn.Dropout(0.40000001)\n",
    "        #2048\n",
    "        self.fc = nn.Linear(384, num_classes)\n",
    "        #self.softmax = nn.LogSoftmax()\n",
    "\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                X = stats.truncnorm(-2, 2, scale=0.01)\n",
    "                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
    "                values = values.view(m.weight.size())\n",
    "                with torch.no_grad():\n",
    "                    m.weight.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _transform_input(self, x):\n",
    "        # type: (Tensor) -> Tensor\n",
    "        if self.transform_input:\n",
    "            x_ch0 = torch.unsqueeze(x[:, 0].clone(), 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "            x_ch1 = torch.unsqueeze(x[:, 1].clone(), 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "            x_ch2 = torch.unsqueeze(x[:, 2].clone(), 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
    "        return x\n",
    "\n",
    "    def _forward(self, x):\n",
    "        # type: (Tensor) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]\n",
    "        # N x 3 x 224 x 224\n",
    "        # already in conv block\n",
    "        x = F.relu(self.conv1(x), inplace = True)\n",
    "        #x = self.conv1(x)\n",
    "        # N x 64 x 112 x 112\n",
    "        x = F.max_pool2d(x, kernel_size = 3, stride = 2, ceil_mode=True) \n",
    "        x = self.lrn(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = F.relu(self.conv2(x), inplace = True)\n",
    "        aux1 = torch.jit.annotate(Optional[Tensor], None)\n",
    "        if self.aux1 is not None:\n",
    "            if self.training:\n",
    "                aux1 = self.aux1(x)\n",
    "        x = self.lrn2(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # N x 64 x 28 x 28\n",
    "        x = self.inception3a(x)\n",
    "        # N x 256 x 28 x 28\n",
    "        \n",
    "        aux2 = torch.jit.annotate(Optional[Tensor], None)\n",
    "        if self.aux2 is not None:\n",
    "            if self.training:\n",
    "                aux2 = self.aux2(x)\n",
    "        # N x 256 x 28 x 28\n",
    "        x = self.avgpool(x)\n",
    "        # N x 1024 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 1024\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        #x = self.softmax(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        return x, aux2, aux1\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def eager_outputs(self, x, aux2, aux1):\n",
    "        # type: (Tensor, Optional[Tensor], Optional[Tensor]) -> GoogLeNetOutputs\n",
    "        if self.training and self.aux_logits:\n",
    "            return _GoogLeNetOutputs(x, aux2, aux1)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # type: (Tensor) -> GoogLeNetOutputs\n",
    "        x = self._transform_input(x)\n",
    "        x, aux1, aux2 = self._forward(x)\n",
    "        aux_defined = self.training and self.aux_logits\n",
    "        if torch.jit.is_scripting():\n",
    "            if not aux_defined:\n",
    "                warnings.warn(\"Scripted GoogleNet always returns GoogleNetOutputs Tuple\")\n",
    "            return GoogLeNetOutputs(x, aux2, aux1)\n",
    "        else:\n",
    "            return self.eager_outputs(x, aux2, aux1)\n",
    "\n",
    "class Inception(nn.Module):\n",
    "\n",
    "   # def __init__(self, in_channels, ch1x1, ch3x3red, pool_proj):\n",
    "   #    super(Inception, self).__init__()\n",
    "        \n",
    "   #    self.branch1 = nn.ReLU(nn.Conv2d(in_channels, ch1x1, kernel_size=1))\n",
    "\n",
    "   #    self.branch2 = nn.ReLU(nn.Conv2d(in_channels, ch3x3red, kernel_size=1))\n",
    "\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, pool_proj,\n",
    "                 conv_block=None):\n",
    "        super(Inception, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "            \n",
    "        self.branch1 = conv_block(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = conv_block(in_channels, ch3x3red, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "\n",
    "\n",
    "        outputs = [branch1, branch2]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, conv_block=None):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.conv = conv_block(in_channels, 128, kernel_size=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14\n",
    "        x = F.adaptive_avg_pool2d(x, (4, 4))\n",
    "        # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4\n",
    "        x = self.conv(x)\n",
    "        # N x 128 x 4 x 4\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 2048\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        # N x 1024\n",
    "        x = F.dropout(x, 0.7, training=self.training)\n",
    "        # N x 1024\n",
    "        x = self.fc2(x)\n",
    "        # N x 1000 (num_classes)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x, inplace=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-0bc76ad473e5>:60: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn('The default weight initialization of GoogleNet will be changed in future releases of '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (lrn): LocalResponseNorm(5, alpha=9.9999997e-05, beta=0.75, k=1.0)\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (lrn2): LocalResponseNorm(5, alpha=9.9999997e-05, beta=0.75, k=1.0)\n",
       "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (inception3a): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=4)\n",
       "  (dropout): Dropout(p=0.40000001, inplace=False)\n",
       "  (fc): Linear(in_features=384, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "network = Network()\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [128, 64, 112, 112]           9,472\n",
      " LocalResponseNorm-2          [128, 64, 56, 56]               0\n",
      "            Conv2d-3          [128, 64, 56, 56]           4,160\n",
      " LocalResponseNorm-4          [128, 64, 56, 56]               0\n",
      "         MaxPool2d-5          [128, 64, 28, 28]               0\n",
      "            Conv2d-6           [128, 8, 28, 28]             512\n",
      "       BatchNorm2d-7           [128, 8, 28, 28]              16\n",
      "       BasicConv2d-8           [128, 8, 28, 28]               0\n",
      "            Conv2d-9          [128, 16, 28, 28]           1,024\n",
      "      BatchNorm2d-10          [128, 16, 28, 28]              32\n",
      "      BasicConv2d-11          [128, 16, 28, 28]               0\n",
      "        Inception-12          [128, 24, 28, 28]               0\n",
      "AdaptiveAvgPool2d-13            [128, 24, 4, 4]               0\n",
      "          Dropout-14                 [128, 384]               0\n",
      "           Linear-15                   [128, 3]           1,155\n",
      "================================================================\n",
      "Total params: 16,371\n",
      "Trainable params: 16,371\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 73.50\n",
      "Forward/backward pass size (MB): 1495.25\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 1568.82\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-0bc76ad473e5>:60: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn('The default weight initialization of GoogleNet will be changed in future releases of '\n"
     ]
    }
   ],
   "source": [
    "model = Network().to(device)\n",
    "summary(model, input_size=(3, 224, 224), batch_size=128, device = str(torch.device(\"cuda\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "      return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the hyper-parameters and return a Run namedtuple containing all the \n",
    "# combinations of hyper-parameters\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "        \n",
    "        Run = namedtuple(\"Run\", params.keys())\n",
    "        \n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "            \n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class, help track loss, accuracy, epoch time, run time, \n",
    "# hyper-parameters etc. Also record to TensorBoard and write into csv, json!@!\n",
    "class RunManager():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.epoch_count = 0\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "        self.epoch_start_time = None\n",
    "        \n",
    "        # tracking every run count, run data, hyper-params used, time\n",
    "        self.run_params = None\n",
    "        self.run_count = 0\n",
    "        self.run_data = []\n",
    "        self.run_start_time = None\n",
    "        \n",
    "        self.network = None\n",
    "        self.loader = None\n",
    "        self.tb = None\n",
    "    \n",
    "    def begin_run(self, run, network, loader):\n",
    "        \n",
    "        self.run_start_time = time.time()\n",
    "        \n",
    "        self.run_params = run\n",
    "        self.run_count += 1\n",
    "        \n",
    "        self.network = network\n",
    "        self.loader = loader\n",
    "        self.tb = SummaryWriter(comment = f'-{run}')\n",
    "        \n",
    "        images, labels = next(iter(self.loader))\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "        \n",
    "        #self.tb.add_image('images', grid)\n",
    "        #self.tb.add_graph(self.network, images)\n",
    "        \n",
    "    def end_run(self):\n",
    "        self.tb.close()\n",
    "        self.epoch_count = 0\n",
    "\n",
    "        \n",
    "        \n",
    "       # zero epoch count, loss, accuracy, \n",
    "    def begin_epoch(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "        self.epoch_count += 1\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "        \n",
    "    # \n",
    "    def end_epoch(self):\n",
    "        # calculate epoch duration and run duration(accumulate)\n",
    "        epoch_duration = time.time() - self.epoch_start_time\n",
    "        run_duration = time.time() - self.run_start_time\n",
    "\n",
    "        # record epoch loss and accuracy\n",
    "        loss = self.epoch_loss / len(self.loader.dataset)\n",
    "        accuracy = self.epoch_num_correct / len(self.loader.dataset)\n",
    "\n",
    "        # Record epoch loss and accuracy to TensorBoard \n",
    "        self.tb.add_scalar('Loss', loss, self.epoch_count)\n",
    "        self.tb.add_scalar('Accuracy', accuracy, self.epoch_count)\n",
    "\n",
    "        # Record params to TensorBoard\n",
    "        for name, param in self.network.named_parameters():\n",
    "            self.tb.add_histogram(name, param, self.epoch_count)\n",
    "            self.tb.add_histogram(f'{name}.grad', param.grad, self.epoch_count)\n",
    "    \n",
    "\n",
    "        # Write into 'results' (OrderedDict) for all run related data\n",
    "        results = OrderedDict()\n",
    "        results[\"run\"] = self.run_count\n",
    "        results[\"epoch\"] = self.epoch_count\n",
    "        results[\"loss\"] = loss\n",
    "        results[\"accuracy\"] = accuracy\n",
    "        results[\"epoch duration\"] = epoch_duration\n",
    "        results[\"run duration\"] = run_duration\n",
    "\n",
    "        # Record hyper-params into 'results'\n",
    "        for k,v in self.run_params._asdict().items(): results[k] = v\n",
    "        self.run_data.append(results)\n",
    "        df = pd.DataFrame.from_dict(self.run_data, orient = 'columns')\n",
    "\n",
    "        # display epoch information and show progress\n",
    "        clear_output(wait=True)\n",
    "        display(df)\n",
    "\n",
    "      # accumulate loss of batch into entire epoch loss\n",
    "    def track_loss(self, loss):\n",
    "        # multiply batch size so variety of batch sizes can be compared\n",
    "        self.epoch_loss += loss.item() * self.loader.batch_size\n",
    "\n",
    "      # accumulate number of corrects of batch into entire epoch num_correct\n",
    "    def track_num_correct(self, preds, labels):\n",
    "        self.epoch_num_correct += self._get_num_correct(preds, labels)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _get_num_correct(self, preds, lables):\n",
    "        return preds.argmax(dim = 1).eq(labels).sum().item()\n",
    "    \n",
    "    def save(self, fileName, model):\n",
    "        \n",
    "        pd.DataFrame.from_dict(\n",
    "        self.run_data,\n",
    "        orient = \"columns\").to_csv(f'{fileName}.csv')\n",
    "        \n",
    "        MODEL_PATH = \"./weights/model_{}.pt\".format(fileName)\n",
    "        torch.save(model, MODEL_PATH)\n",
    "        \n",
    "        with open(f'{fileName}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.run_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.ImageFolder(\n",
    "    root = './Data_cleaned/train'\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 253 ms, sys: 221 ms, total: 474 ms\n",
      "Wall time: 2.03 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.4738), tensor(0.2598))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size = 512, num_workers = 2, pin_memory = True\n",
    ")\n",
    "data = next(iter(loader))\n",
    "mean = data[0].mean()\n",
    "std = data[0].std()\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_normal = torchvision.datasets.ImageFolder(\n",
    "    root = './Data_cleaned/train'\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsets = {\n",
    "    'not_normal': train_set,\n",
    "    'normal' : train_set_normal\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-9bea0d24b132>:61: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn('The default weight initialization of GoogleNet will be changed in future releases of '\n"
     ]
    }
   ],
   "source": [
    "networkBN = NetworkBN()\n",
    "#network2 = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = {\n",
    "#    'no_batch_norm': network,\n",
    "    'batch_norm': networkBN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIR = '/home/vadim/testovoe/dev/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(CURRENT_DIR+\"Data/text.txt\", sep=\"\\s+\", header=None, names=[\"name\", \"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_width(im, new_shape, is_rgb=True):\n",
    "    pad_diff = new_shape - im.shape[0], new_shape - im.shape[1]\n",
    "    t, b = math.floor(pad_diff[0]/2), math.ceil(pad_diff[0]/2)\n",
    "    l, r = math.floor(pad_diff[1]/2), math.ceil(pad_diff[1]/2)\n",
    "    if is_rgb:\n",
    "        pad_width = ((t,b), (l,r), (0, 0))\n",
    "    else:\n",
    "        pad_width = ((t,b), (l,r))\n",
    "    return pad_width\n",
    "\n",
    "def preprocess_image(image_path, desired_size=224):\n",
    "    im = Image.open(image_path)\n",
    "    im = im.resize((desired_size, )*2, resample=Image.LANCZOS)\n",
    "    \n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:00<00:00, 1083.95it/s]\n"
     ]
    }
   ],
   "source": [
    "N = test_df.shape[0]\n",
    "x_test = np.empty((N, 224, 224, 3), dtype=np.uint8)\n",
    "\n",
    "for i, image_id in enumerate(tqdm(test_df['name'])):\n",
    "    x_test[i, :, :, :] = preprocess_image(\n",
    "         f'/home/vadim/testovoe/dev/{image_id}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.get_dummies(test_df['category']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vadim/anaconda3/envs/pytorch/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/vadim/anaconda3/envs/pytorch/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/vadim/anaconda3/envs/pytorch/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/vadim/anaconda3/envs/pytorch/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "Traceback (most recent call last):\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/vadim/anaconda3/envs/pytorch/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/vadim/anaconda3/envs/pytorch/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/vadim/anaconda3/envs/pytorch/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/vadim/anaconda3/envs/pytorch/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test different configurations\n",
    "# for every run [value] that is going to be used e.g [.001, .01] = two runs\n",
    "# the bigger the batch the less gradient updates steps made\n",
    "params = OrderedDict(\n",
    "    lr = [.001],\n",
    "    batch_size = [352],\n",
    "    num_workers = [2],\n",
    "    device = [\"cuda\"],\n",
    "    trainset = [\"normal\"],\n",
    "    # try all the values in the dict network1, network2\n",
    "    network = list(networks.keys())\n",
    ")\n",
    "m = RunManager()\n",
    "# active run or current run\n",
    "# this thing detecting RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for run in RunBuilder.get_runs(params):\n",
    "    \n",
    "    device = torch.device(run.device)\n",
    "    # redefine the network\n",
    "    network = networks[run.network].to(device)\n",
    "    loader = DataLoader(trainsets[run.trainset], batch_size = run.batch_size, num_workers = run.num_workers, \n",
    "                       pin_memory = True) \n",
    "    optimizer = optim.Adam(network.parameters(), lr = run.lr) \n",
    "    \n",
    "    m.begin_run(run, network, loader)\n",
    "    for epoch in range(60):\n",
    "        m.begin_epoch()\n",
    "        for batch in loader:\n",
    "            #network.train()\n",
    "            images = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            preds = network(images)\n",
    "            loss = F.cross_entropy(preds, labels)\n",
    "            #7.9\n",
    "            #optimizer.zero_grad()\n",
    "            #8 sec\n",
    "            for p in network.parameters(): p.grad = None\n",
    "            loss.backward() # Calculate gradients\n",
    "            optimizer.step() # Update Weights\n",
    "            m.track_loss(loss)\n",
    "            m.track_num_correct(preds, labels)\n",
    "        m.end_epoch()\n",
    "    m.end_run()\n",
    "m.save(\"results\", network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./weights/model_results.pt\"\n",
    "model = torch.load(MODEL_PATH)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>num_workers</th>\n",
       "      <th>device</th>\n",
       "      <th>trainset</th>\n",
       "      <th>network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0.230396</td>\n",
       "      <td>0.924114</td>\n",
       "      <td>32.205194</td>\n",
       "      <td>2025.794931</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>0.245730</td>\n",
       "      <td>0.916186</td>\n",
       "      <td>32.658620</td>\n",
       "      <td>1993.497099</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>0.261080</td>\n",
       "      <td>0.910541</td>\n",
       "      <td>33.393565</td>\n",
       "      <td>1960.749425</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>0.275600</td>\n",
       "      <td>0.904464</td>\n",
       "      <td>31.858836</td>\n",
       "      <td>1927.258806</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>0.292315</td>\n",
       "      <td>0.901040</td>\n",
       "      <td>33.516353</td>\n",
       "      <td>1895.298787</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>0.310293</td>\n",
       "      <td>0.891137</td>\n",
       "      <td>32.374610</td>\n",
       "      <td>1861.696040</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>0.322427</td>\n",
       "      <td>0.886078</td>\n",
       "      <td>32.034346</td>\n",
       "      <td>1829.232883</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.343567</td>\n",
       "      <td>0.875096</td>\n",
       "      <td>33.513906</td>\n",
       "      <td>1797.106297</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0.365850</td>\n",
       "      <td>0.866120</td>\n",
       "      <td>32.100678</td>\n",
       "      <td>1763.504941</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>0.388128</td>\n",
       "      <td>0.856557</td>\n",
       "      <td>34.180029</td>\n",
       "      <td>1731.307720</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.425690</td>\n",
       "      <td>0.839868</td>\n",
       "      <td>32.237083</td>\n",
       "      <td>1697.041225</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>0.443112</td>\n",
       "      <td>0.834346</td>\n",
       "      <td>35.610597</td>\n",
       "      <td>1632.067805</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>0.451896</td>\n",
       "      <td>0.830768</td>\n",
       "      <td>32.554191</td>\n",
       "      <td>1664.709117</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0.465539</td>\n",
       "      <td>0.825616</td>\n",
       "      <td>34.610935</td>\n",
       "      <td>1596.365132</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0.477557</td>\n",
       "      <td>0.823364</td>\n",
       "      <td>36.551774</td>\n",
       "      <td>1561.663461</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.488009</td>\n",
       "      <td>0.816516</td>\n",
       "      <td>40.490910</td>\n",
       "      <td>1525.020440</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.814511</td>\n",
       "      <td>31.872526</td>\n",
       "      <td>1484.445340</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>0.500213</td>\n",
       "      <td>0.807169</td>\n",
       "      <td>32.097206</td>\n",
       "      <td>1452.488097</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>0.516587</td>\n",
       "      <td>0.801771</td>\n",
       "      <td>34.126576</td>\n",
       "      <td>1420.303415</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.550819</td>\n",
       "      <td>0.781997</td>\n",
       "      <td>31.911367</td>\n",
       "      <td>1386.090878</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0.568313</td>\n",
       "      <td>0.776876</td>\n",
       "      <td>32.524822</td>\n",
       "      <td>1354.093094</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0.585498</td>\n",
       "      <td>0.767591</td>\n",
       "      <td>32.410193</td>\n",
       "      <td>1321.482593</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0.623441</td>\n",
       "      <td>0.755961</td>\n",
       "      <td>33.378484</td>\n",
       "      <td>1257.164683</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0.608181</td>\n",
       "      <td>0.755375</td>\n",
       "      <td>31.745247</td>\n",
       "      <td>1288.990943</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0.647245</td>\n",
       "      <td>0.738625</td>\n",
       "      <td>31.738088</td>\n",
       "      <td>1223.701902</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>0.681528</td>\n",
       "      <td>0.721874</td>\n",
       "      <td>31.964564</td>\n",
       "      <td>1191.885967</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0.710472</td>\n",
       "      <td>0.705278</td>\n",
       "      <td>33.611010</td>\n",
       "      <td>1159.837166</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0.746405</td>\n",
       "      <td>0.687972</td>\n",
       "      <td>31.727303</td>\n",
       "      <td>1126.141076</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.767823</td>\n",
       "      <td>0.673721</td>\n",
       "      <td>31.871119</td>\n",
       "      <td>1094.334812</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0.774881</td>\n",
       "      <td>0.670358</td>\n",
       "      <td>33.690905</td>\n",
       "      <td>1062.383955</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.800354</td>\n",
       "      <td>0.655520</td>\n",
       "      <td>31.617499</td>\n",
       "      <td>1028.613491</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0.822188</td>\n",
       "      <td>0.646451</td>\n",
       "      <td>31.715499</td>\n",
       "      <td>932.310636</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.816379</td>\n",
       "      <td>0.645094</td>\n",
       "      <td>31.712403</td>\n",
       "      <td>996.910454</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0.828122</td>\n",
       "      <td>0.642687</td>\n",
       "      <td>35.111930</td>\n",
       "      <td>830.521672</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.830031</td>\n",
       "      <td>0.637382</td>\n",
       "      <td>32.732260</td>\n",
       "      <td>965.121439</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.837067</td>\n",
       "      <td>0.628528</td>\n",
       "      <td>35.651042</td>\n",
       "      <td>866.247224</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.334561</td>\n",
       "      <td>0.623037</td>\n",
       "      <td>35.898659</td>\n",
       "      <td>37.670180</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.872040</td>\n",
       "      <td>0.611408</td>\n",
       "      <td>34.185139</td>\n",
       "      <td>900.514676</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.885990</td>\n",
       "      <td>0.602708</td>\n",
       "      <td>34.419141</td>\n",
       "      <td>795.333850</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0.891775</td>\n",
       "      <td>0.597403</td>\n",
       "      <td>37.661122</td>\n",
       "      <td>723.402500</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.882214</td>\n",
       "      <td>0.593855</td>\n",
       "      <td>37.363839</td>\n",
       "      <td>760.843105</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.918076</td>\n",
       "      <td>0.579758</td>\n",
       "      <td>35.012072</td>\n",
       "      <td>685.660070</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.942070</td>\n",
       "      <td>0.556529</td>\n",
       "      <td>35.792543</td>\n",
       "      <td>617.577238</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.939544</td>\n",
       "      <td>0.555974</td>\n",
       "      <td>32.900485</td>\n",
       "      <td>650.574430</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.943468</td>\n",
       "      <td>0.551840</td>\n",
       "      <td>36.537850</td>\n",
       "      <td>581.704958</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.966760</td>\n",
       "      <td>0.541475</td>\n",
       "      <td>33.417240</td>\n",
       "      <td>510.554208</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.965759</td>\n",
       "      <td>0.541413</td>\n",
       "      <td>34.445150</td>\n",
       "      <td>545.084210</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.955431</td>\n",
       "      <td>0.538946</td>\n",
       "      <td>36.243114</td>\n",
       "      <td>477.064112</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.996310</td>\n",
       "      <td>0.502206</td>\n",
       "      <td>31.933825</td>\n",
       "      <td>405.671968</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1.026307</td>\n",
       "      <td>0.478854</td>\n",
       "      <td>35.000565</td>\n",
       "      <td>440.743878</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1.038649</td>\n",
       "      <td>0.463738</td>\n",
       "      <td>32.375952</td>\n",
       "      <td>373.662963</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1.076554</td>\n",
       "      <td>0.417096</td>\n",
       "      <td>32.899455</td>\n",
       "      <td>341.205771</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.691512</td>\n",
       "      <td>0.400839</td>\n",
       "      <td>35.176102</td>\n",
       "      <td>72.916034</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.459556</td>\n",
       "      <td>0.392109</td>\n",
       "      <td>37.293684</td>\n",
       "      <td>110.273176</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1.110357</td>\n",
       "      <td>0.384891</td>\n",
       "      <td>31.700728</td>\n",
       "      <td>308.229974</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1.142037</td>\n",
       "      <td>0.382515</td>\n",
       "      <td>31.920732</td>\n",
       "      <td>276.452563</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1.204801</td>\n",
       "      <td>0.364284</td>\n",
       "      <td>33.824100</td>\n",
       "      <td>244.447036</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1.209176</td>\n",
       "      <td>0.359534</td>\n",
       "      <td>32.109801</td>\n",
       "      <td>210.546810</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.438451</td>\n",
       "      <td>0.344912</td>\n",
       "      <td>36.107962</td>\n",
       "      <td>146.445998</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.303357</td>\n",
       "      <td>0.337570</td>\n",
       "      <td>31.840595</td>\n",
       "      <td>178.365510</td>\n",
       "      <td>0.001</td>\n",
       "      <td>352</td>\n",
       "      <td>2</td>\n",
       "      <td>cuda</td>\n",
       "      <td>normal</td>\n",
       "      <td>batch_norm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    run  epoch      loss  accuracy  epoch duration  run duration     lr  \\\n",
       "59    1     60  0.230396  0.924114       32.205194   2025.794931  0.001   \n",
       "58    1     59  0.245730  0.916186       32.658620   1993.497099  0.001   \n",
       "57    1     58  0.261080  0.910541       33.393565   1960.749425  0.001   \n",
       "56    1     57  0.275600  0.904464       31.858836   1927.258806  0.001   \n",
       "55    1     56  0.292315  0.901040       33.516353   1895.298787  0.001   \n",
       "54    1     55  0.310293  0.891137       32.374610   1861.696040  0.001   \n",
       "53    1     54  0.322427  0.886078       32.034346   1829.232883  0.001   \n",
       "52    1     53  0.343567  0.875096       33.513906   1797.106297  0.001   \n",
       "51    1     52  0.365850  0.866120       32.100678   1763.504941  0.001   \n",
       "50    1     51  0.388128  0.856557       34.180029   1731.307720  0.001   \n",
       "49    1     50  0.425690  0.839868       32.237083   1697.041225  0.001   \n",
       "47    1     48  0.443112  0.834346       35.610597   1632.067805  0.001   \n",
       "48    1     49  0.451896  0.830768       32.554191   1664.709117  0.001   \n",
       "46    1     47  0.465539  0.825616       34.610935   1596.365132  0.001   \n",
       "45    1     46  0.477557  0.823364       36.551774   1561.663461  0.001   \n",
       "44    1     45  0.488009  0.816516       40.490910   1525.020440  0.001   \n",
       "43    1     44  0.484536  0.814511       31.872526   1484.445340  0.001   \n",
       "42    1     43  0.500213  0.807169       32.097206   1452.488097  0.001   \n",
       "41    1     42  0.516587  0.801771       34.126576   1420.303415  0.001   \n",
       "40    1     41  0.550819  0.781997       31.911367   1386.090878  0.001   \n",
       "39    1     40  0.568313  0.776876       32.524822   1354.093094  0.001   \n",
       "38    1     39  0.585498  0.767591       32.410193   1321.482593  0.001   \n",
       "36    1     37  0.623441  0.755961       33.378484   1257.164683  0.001   \n",
       "37    1     38  0.608181  0.755375       31.745247   1288.990943  0.001   \n",
       "35    1     36  0.647245  0.738625       31.738088   1223.701902  0.001   \n",
       "34    1     35  0.681528  0.721874       31.964564   1191.885967  0.001   \n",
       "33    1     34  0.710472  0.705278       33.611010   1159.837166  0.001   \n",
       "32    1     33  0.746405  0.687972       31.727303   1126.141076  0.001   \n",
       "31    1     32  0.767823  0.673721       31.871119   1094.334812  0.001   \n",
       "30    1     31  0.774881  0.670358       33.690905   1062.383955  0.001   \n",
       "29    1     30  0.800354  0.655520       31.617499   1028.613491  0.001   \n",
       "26    1     27  0.822188  0.646451       31.715499    932.310636  0.001   \n",
       "28    1     29  0.816379  0.645094       31.712403    996.910454  0.001   \n",
       "23    1     24  0.828122  0.642687       35.111930    830.521672  0.001   \n",
       "27    1     28  0.830031  0.637382       32.732260    965.121439  0.001   \n",
       "24    1     25  0.837067  0.628528       35.651042    866.247224  0.001   \n",
       "0     1      1  1.334561  0.623037       35.898659     37.670180  0.001   \n",
       "25    1     26  0.872040  0.611408       34.185139    900.514676  0.001   \n",
       "22    1     23  0.885990  0.602708       34.419141    795.333850  0.001   \n",
       "20    1     21  0.891775  0.597403       37.661122    723.402500  0.001   \n",
       "21    1     22  0.882214  0.593855       37.363839    760.843105  0.001   \n",
       "19    1     20  0.918076  0.579758       35.012072    685.660070  0.001   \n",
       "17    1     18  0.942070  0.556529       35.792543    617.577238  0.001   \n",
       "18    1     19  0.939544  0.555974       32.900485    650.574430  0.001   \n",
       "16    1     17  0.943468  0.551840       36.537850    581.704958  0.001   \n",
       "14    1     15  0.966760  0.541475       33.417240    510.554208  0.001   \n",
       "15    1     16  0.965759  0.541413       34.445150    545.084210  0.001   \n",
       "13    1     14  0.955431  0.538946       36.243114    477.064112  0.001   \n",
       "11    1     12  0.996310  0.502206       31.933825    405.671968  0.001   \n",
       "12    1     13  1.026307  0.478854       35.000565    440.743878  0.001   \n",
       "10    1     11  1.038649  0.463738       32.375952    373.662963  0.001   \n",
       "9     1     10  1.076554  0.417096       32.899455    341.205771  0.001   \n",
       "1     1      2  1.691512  0.400839       35.176102     72.916034  0.001   \n",
       "2     1      3  1.459556  0.392109       37.293684    110.273176  0.001   \n",
       "8     1      9  1.110357  0.384891       31.700728    308.229974  0.001   \n",
       "7     1      8  1.142037  0.382515       31.920732    276.452563  0.001   \n",
       "6     1      7  1.204801  0.364284       33.824100    244.447036  0.001   \n",
       "5     1      6  1.209176  0.359534       32.109801    210.546810  0.001   \n",
       "3     1      4  1.438451  0.344912       36.107962    146.445998  0.001   \n",
       "4     1      5  1.303357  0.337570       31.840595    178.365510  0.001   \n",
       "\n",
       "    batch_size  num_workers device trainset     network  \n",
       "59         352            2   cuda   normal  batch_norm  \n",
       "58         352            2   cuda   normal  batch_norm  \n",
       "57         352            2   cuda   normal  batch_norm  \n",
       "56         352            2   cuda   normal  batch_norm  \n",
       "55         352            2   cuda   normal  batch_norm  \n",
       "54         352            2   cuda   normal  batch_norm  \n",
       "53         352            2   cuda   normal  batch_norm  \n",
       "52         352            2   cuda   normal  batch_norm  \n",
       "51         352            2   cuda   normal  batch_norm  \n",
       "50         352            2   cuda   normal  batch_norm  \n",
       "49         352            2   cuda   normal  batch_norm  \n",
       "47         352            2   cuda   normal  batch_norm  \n",
       "48         352            2   cuda   normal  batch_norm  \n",
       "46         352            2   cuda   normal  batch_norm  \n",
       "45         352            2   cuda   normal  batch_norm  \n",
       "44         352            2   cuda   normal  batch_norm  \n",
       "43         352            2   cuda   normal  batch_norm  \n",
       "42         352            2   cuda   normal  batch_norm  \n",
       "41         352            2   cuda   normal  batch_norm  \n",
       "40         352            2   cuda   normal  batch_norm  \n",
       "39         352            2   cuda   normal  batch_norm  \n",
       "38         352            2   cuda   normal  batch_norm  \n",
       "36         352            2   cuda   normal  batch_norm  \n",
       "37         352            2   cuda   normal  batch_norm  \n",
       "35         352            2   cuda   normal  batch_norm  \n",
       "34         352            2   cuda   normal  batch_norm  \n",
       "33         352            2   cuda   normal  batch_norm  \n",
       "32         352            2   cuda   normal  batch_norm  \n",
       "31         352            2   cuda   normal  batch_norm  \n",
       "30         352            2   cuda   normal  batch_norm  \n",
       "29         352            2   cuda   normal  batch_norm  \n",
       "26         352            2   cuda   normal  batch_norm  \n",
       "28         352            2   cuda   normal  batch_norm  \n",
       "23         352            2   cuda   normal  batch_norm  \n",
       "27         352            2   cuda   normal  batch_norm  \n",
       "24         352            2   cuda   normal  batch_norm  \n",
       "0          352            2   cuda   normal  batch_norm  \n",
       "25         352            2   cuda   normal  batch_norm  \n",
       "22         352            2   cuda   normal  batch_norm  \n",
       "20         352            2   cuda   normal  batch_norm  \n",
       "21         352            2   cuda   normal  batch_norm  \n",
       "19         352            2   cuda   normal  batch_norm  \n",
       "17         352            2   cuda   normal  batch_norm  \n",
       "18         352            2   cuda   normal  batch_norm  \n",
       "16         352            2   cuda   normal  batch_norm  \n",
       "14         352            2   cuda   normal  batch_norm  \n",
       "15         352            2   cuda   normal  batch_norm  \n",
       "13         352            2   cuda   normal  batch_norm  \n",
       "11         352            2   cuda   normal  batch_norm  \n",
       "12         352            2   cuda   normal  batch_norm  \n",
       "10         352            2   cuda   normal  batch_norm  \n",
       "9          352            2   cuda   normal  batch_norm  \n",
       "1          352            2   cuda   normal  batch_norm  \n",
       "2          352            2   cuda   normal  batch_norm  \n",
       "8          352            2   cuda   normal  batch_norm  \n",
       "7          352            2   cuda   normal  batch_norm  \n",
       "6          352            2   cuda   normal  batch_norm  \n",
       "5          352            2   cuda   normal  batch_norm  \n",
       "3          352            2   cuda   normal  batch_norm  \n",
       "4          352            2   cuda   normal  batch_norm  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(m.run_data).sort_values(\"accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=[x.strip() for x in open('labels_adam.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 373.20 fps\n",
      "CPU times: user 2.64 s, sys: 40.5 ms, total: 2.68 s\n",
      "Wall time: 2.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This could be done by defining separate dataloader for test dataset\n",
    "preds_adam = []\n",
    "preds = []\n",
    "from timeit import default_timer as timer\n",
    "time_start = timer()\n",
    "device = torch.device(\"cuda\")\n",
    "# this is untrained\n",
    "model = networks['batch_norm'].to(device)\n",
    "#model = Network().to(device)\n",
    "def get_preds(network):\n",
    "        network = network\n",
    "        # this is very ineffective O(n^2)+complexity of network\n",
    "        # model.eval() will notify all your layers that you are in eval mode, that way, \n",
    "        # batchnorm or dropout layers will work in eval mode instead of training mode.\n",
    "        # torch.no_grad() impacts the autograd engine and deactivate it. \n",
    "        # It will reduce memory usage and speed up computations but you won’t be able to backprop \n",
    "        # (which you don’t want in an eval script).\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, x in enumerate(categories):\n",
    "                for j, y in enumerate(sorted(glob.glob('Data/test/{}/*'.format(x)))):\n",
    "                    im = Image.open(y)\n",
    "                    image = transforms.ToTensor()(im).unsqueeze_(0).to(device)\n",
    "                    output = model(image) # preds\n",
    "                    preds = output.argmax(dim=1, keepdim=True)\n",
    "                    preds_adam.extend(preds.cpu().numpy())\n",
    "                    #preds_adam.append(preds.detach().cpu().clone().numpy())\n",
    "                    #print(preds_adam)\n",
    "                    #preds_np = pred.detach().cpu().clone().numpy()\n",
    "                    #print(preds_np)\n",
    "                    #preds_np.append(pred.detach().cpu().clone().numpy())\n",
    "                    #preds_adam.extend(pred.cpu().numpy())\n",
    "                    #print(preds)\n",
    "                    #print(preds.argmax(dim = 1))\n",
    "                    #preds_np.append(preds.detach().cpu().clone().numpy())\n",
    "                    #preds_adam.append(np.argmax(preds_np))\n",
    "                    #print(y, categories[np.argmax(pred['softmax'])])\n",
    "        return preds_adam\n",
    "preds_adam = get_preds(network)\n",
    "time_end = timer()\n",
    "fps=('FPS: %.2f fps' % (1000/(time_end-time_start)))\n",
    "print('FPS: %.2f fps' % (1000/(time_end-time_start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2])]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDigits(lst): \n",
    "    return list(map(lambda el:el.tolist(), lst)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([0]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2]),\n",
       " array([2])]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractDigits(preds_adam)\n",
    "preds_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in preds_adam for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = LabelBinarizer()\n",
    "preds_adam = lb.fit_transform(flat_list)\n",
    "preds_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.60      0.73       300\n",
      "           1       0.76      0.49      0.60       300\n",
      "           2       0.56      0.95      0.70       300\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       900\n",
      "   macro avg       0.75      0.68      0.68       900\n",
      "weighted avg       0.75      0.68      0.68       900\n",
      " samples avg       0.68      0.68      0.68       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds_adam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7874074074074074\n"
     ]
    }
   ],
   "source": [
    "mAP_adam = str(average_precision_score(y_test, preds_adam, average=\"samples\"))\n",
    "print(average_precision_score(y_test, preds_adam, average=\"samples\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
